{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2e9b7fe",
   "metadata": {},
   "source": [
    "Neural Networks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fa0f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded test_data_h1: (2571058, 176)\n",
      "Columns: ['ID', 'iv_cw', 'date', 'cp_flag', 'option_price', 'stock_price', 'moneyness', 'time_to_expiry', 'strike_price', 'volume']...\n",
      "--------------------------------------------------\n",
      "Loaded test_data_h5: (2203398, 176)\n",
      "Columns: ['ID', 'iv_cw', 'date', 'cp_flag', 'option_price', 'stock_price', 'moneyness', 'time_to_expiry', 'strike_price', 'volume']...\n",
      "--------------------------------------------------\n",
      "Loaded test_data_h21: (1341910, 176)\n",
      "Columns: ['ID', 'iv_cw', 'date', 'cp_flag', 'option_price', 'stock_price', 'moneyness', 'time_to_expiry', 'strike_price', 'volume']...\n",
      "--------------------------------------------------\n",
      "\n",
      "Option-only data for horizon 1:\n",
      "Shape: (2571058, 176)\n",
      "                              ID     iv_cw        date  cp_flag  option_price  \\\n",
      "0  2017-01-03_AAPL 170127C116000  0.264812  2017-01-03        1         2.595   \n",
      "1  2017-01-03_AAPL 170127C117000  0.266365  2017-01-03        1         2.110   \n",
      "2  2017-01-03_AAPL 170127C118000  0.269537  2017-01-03        1         1.680   \n",
      "3  2017-01-03_AAPL 170127C119000  0.274232  2017-01-03        1         1.315   \n",
      "4  2017-01-03_AAPL 170127C120000  0.280334  2017-01-03        1         1.010   \n",
      "\n",
      "   stock_price  moneyness  time_to_expiry  strike_price  volume  ...  \\\n",
      "0      115.545   0.996078              23         116.0     885  ...   \n",
      "1      115.545   0.987564              23         117.0     579  ...   \n",
      "2      115.545   0.979195              23         118.0     142  ...   \n",
      "3      115.545   0.970966              23         119.0     303  ...   \n",
      "4      115.545   0.962875              23         120.0     954  ...   \n",
      "\n",
      "    iv_ahbs     iv_bs  prediction_horizon  train_date   test_date  \\\n",
      "0  0.299345  0.288808                 h=1  2017-01-03  2017-01-04   \n",
      "1  0.298947  0.288808                 h=1  2017-01-03  2017-01-04   \n",
      "2  0.298851  0.288808                 h=1  2017-01-03  2017-01-04   \n",
      "3  0.299042  0.288808                 h=1  2017-01-03  2017-01-04   \n",
      "4  0.299507  0.288808                 h=1  2017-01-03  2017-01-04   \n",
      "\n",
      "                          new_id  impl_volatility  iv_bs_error  iv_ahbs_error  \\\n",
      "0  2017-01-04_AAPL 170127C116000         0.201081    -0.087727      -0.098264   \n",
      "1  2017-01-04_AAPL 170127C117000         0.196421    -0.092387      -0.102526   \n",
      "2  2017-01-04_AAPL 170127C118000         0.191262    -0.097546      -0.107589   \n",
      "3  2017-01-04_AAPL 170127C119000         0.188833    -0.099975      -0.110209   \n",
      "4  2017-01-04_AAPL 170127C120000         0.188554    -0.100254      -0.110953   \n",
      "\n",
      "   iv_cw_error  \n",
      "0    -0.063731  \n",
      "1    -0.069944  \n",
      "2    -0.078275  \n",
      "3    -0.085399  \n",
      "4    -0.091780  \n",
      "\n",
      "[5 rows x 176 columns]\n",
      "--------------------------------------------------\n",
      "\n",
      "Option-only data for horizon 5:\n",
      "Shape: (2203398, 176)\n",
      "                              ID     iv_cw        date  cp_flag  option_price  \\\n",
      "0  2017-01-03_AAPL 170203C116000  0.263030  2017-01-03        1         3.200   \n",
      "1  2017-01-03_AAPL 170203C117000  0.264520  2017-01-03        1         2.680   \n",
      "2  2017-01-03_AAPL 170203C118000  0.267565  2017-01-03        1         2.245   \n",
      "3  2017-01-03_AAPL 170203C119000  0.272075  2017-01-03        1         1.855   \n",
      "4  2017-01-03_AAPL 170203C120000  0.277940  2017-01-03        1         1.505   \n",
      "\n",
      "   stock_price  moneyness  time_to_expiry  strike_price  volume  ...  \\\n",
      "0      115.545   0.996078              26         116.0     149  ...   \n",
      "1      115.545   0.987564              26         117.0     520  ...   \n",
      "2      115.545   0.979195              26         118.0     243  ...   \n",
      "3      115.545   0.970966              26         119.0     157  ...   \n",
      "4      115.545   0.962875              26         120.0     650  ...   \n",
      "\n",
      "    iv_ahbs     iv_bs  prediction_horizon  train_date   test_date  \\\n",
      "0  0.296150  0.288808                 h=5  2017-01-03  2017-01-10   \n",
      "1  0.295814  0.288808                 h=5  2017-01-03  2017-01-10   \n",
      "2  0.295779  0.288808                 h=5  2017-01-03  2017-01-10   \n",
      "3  0.296031  0.288808                 h=5  2017-01-03  2017-01-10   \n",
      "4  0.296556  0.288808                 h=5  2017-01-03  2017-01-10   \n",
      "\n",
      "                          new_id  impl_volatility  iv_bs_error  iv_ahbs_error  \\\n",
      "0  2017-01-10_AAPL 170203C116000         0.246879    -0.041929      -0.049271   \n",
      "1  2017-01-10_AAPL 170203C117000         0.243913    -0.044895      -0.051901   \n",
      "2  2017-01-10_AAPL 170203C118000         0.239469    -0.049339      -0.056310   \n",
      "3  2017-01-10_AAPL 170203C119000         0.236722    -0.052086      -0.059309   \n",
      "4  2017-01-10_AAPL 170203C120000         0.236068    -0.052740      -0.060488   \n",
      "\n",
      "   iv_cw_error  \n",
      "0    -0.016151  \n",
      "1    -0.020607  \n",
      "2    -0.028096  \n",
      "3    -0.035353  \n",
      "4    -0.041872  \n",
      "\n",
      "[5 rows x 176 columns]\n",
      "--------------------------------------------------\n",
      "\n",
      "Option-only data for horizon 21:\n",
      "Shape: (1341910, 176)\n",
      "                              ID     iv_cw        date  cp_flag  option_price  \\\n",
      "0  2017-01-03_AAPL 170317C120000  0.261731  2017-01-03        1         2.440   \n",
      "1  2017-01-03_AAPL 170317C125000  0.294981  2017-01-03        1         1.100   \n",
      "2  2017-01-03_AAPL 170317C130000  0.342147  2017-01-03        1         0.465   \n",
      "3  2017-01-03_AAPL 170317C135000  0.396596  2017-01-03        1         0.200   \n",
      "4  2017-01-03_AAPL 170317P100000  0.380884  2017-01-03        0         0.615   \n",
      "\n",
      "   stock_price  moneyness  time_to_expiry  strike_price  volume  ...  \\\n",
      "0      115.545   0.962875              52         120.0    1369  ...   \n",
      "1      115.545   0.924360              52         125.0    1329  ...   \n",
      "2      115.545   0.888808              52         130.0     268  ...   \n",
      "3      115.545   0.855889              52         135.0     201  ...   \n",
      "4      115.545   1.155450              52         100.0     313  ...   \n",
      "\n",
      "    iv_ahbs     iv_bs  prediction_horizon  train_date   test_date  \\\n",
      "0  0.274520  0.288808                h=21  2017-01-03  2017-02-02   \n",
      "1  0.283228  0.288808                h=21  2017-01-03  2017-02-02   \n",
      "2  0.296779  0.288808                h=21  2017-01-03  2017-02-02   \n",
      "3  0.314044  0.288808                h=21  2017-01-03  2017-02-02   \n",
      "4  0.324136  0.288808                h=21  2017-01-03  2017-02-02   \n",
      "\n",
      "                          new_id  impl_volatility  iv_bs_error  iv_ahbs_error  \\\n",
      "0  2017-02-02_AAPL 170317C120000         0.159662    -0.129146      -0.114858   \n",
      "1  2017-02-02_AAPL 170317C125000         0.153761    -0.135047      -0.129467   \n",
      "2  2017-02-02_AAPL 170317C130000         0.146285    -0.142523      -0.150494   \n",
      "3  2017-02-02_AAPL 170317C135000         0.145885    -0.142923      -0.168159   \n",
      "4  2017-02-02_AAPL 170317P100000         0.329168     0.040360       0.005032   \n",
      "\n",
      "   iv_cw_error  \n",
      "0    -0.102069  \n",
      "1    -0.141220  \n",
      "2    -0.195862  \n",
      "3    -0.250711  \n",
      "4    -0.051716  \n",
      "\n",
      "[5 rows x 176 columns]\n",
      "--------------------------------------------------\n",
      "\n",
      "Train data:\n",
      "Shape: (2669654, 166)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, load_model, save_model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.inspection import permutation_importance as sklearn_permutation_importance\n",
    "\n",
    "\n",
    "\n",
    "# Create a dictionary to store data for each horizon\n",
    "test_data = {}\n",
    "horizons = [1, 5, 21]\n",
    "\n",
    "# Load test data for each horizon\n",
    "for horizon in horizons:\n",
    "    # Define the file path\n",
    "    file_path = f'C:/Users/maxva/OneDrive - Tilburg University/Msc. Data Science/Master Thesis/Data/merged_results_h{horizon}.csv'\n",
    "    \n",
    "    # Load the data and store it in the dictionary\n",
    "    test_data[horizon] = pd.read_csv(file_path)\n",
    "    \n",
    "    # Print confirmation and info about the loaded data\n",
    "    print(f\"Loaded test_data_h{horizon}: {test_data[horizon].shape}\")\n",
    "    print(f\"Columns: {test_data[horizon].columns.tolist()[:10]}...\")  # First 10 columns\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "\n",
    "\n",
    "# Create option-only datasets for each horizon\n",
    "option_only_data = {}\n",
    "for horizon in horizons:\n",
    "    test_data[horizon] = test_data[horizon].copy()\n",
    "    print(f\"\\nOption-only data for horizon {horizon}:\")\n",
    "    print(f\"Shape: {test_data[horizon].shape}\")\n",
    "    print(test_data[horizon].head())\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "train_data = pd.read_csv('C:/Users/maxva/OneDrive - Tilburg University/Msc. Data Science/Master Thesis/Data/merged_results_train.csv')\n",
    "# Drop the date column\n",
    "train_data = train_data.drop(columns=['date', 'moneyness_category'])\n",
    "train_data = train_data\n",
    "print(f\"\\nTrain data:\")\n",
    "print(f\"Shape: {train_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d70c24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################\n",
    "# PART 1: NEURAL NETWORK MODEL DEFINITION\n",
    "###########################################\n",
    "\n",
    "def create_nn_model(architecture_type, input_dim):\n",
    "\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Define architecture based on specifications\n",
    "    if architecture_type == 'NN3':\n",
    "        # Three hidden layers with 32, 16, and 8 neurons\n",
    "        model.add(Dense(128, activation='sigmoid', input_dim=input_dim))\n",
    "        model.add(Dense(64, activation='sigmoid'))\n",
    "        model.add(Dense(16, activation='sigmoid'))\n",
    "        model.add(Dense(1))\n",
    "    \n",
    "    elif architecture_type == 'NN4':\n",
    "        # Four hidden layers with 32, 16, 8, and 4 neurons\n",
    "        model.add(Dense(128, activation='sigmoid', input_dim=input_dim))\n",
    "        model.add(Dense(64, activation='sigmoid'))\n",
    "        model.add(Dense(32, activation='sigmoid'))\n",
    "        model.add(Dense(16, activation='sigmoid'))\n",
    "        model.add(Dense(1))\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"Invalid architecture type. Choose from 'NN3' or 'NN4'.\")\n",
    "    \n",
    "    # Compile model with Adam optimizer for faster training\n",
    "    model.compile(optimizer=Adam(learning_rate=0.01), loss='mean_squared_error')\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_and_evaluate_model(model, X_train_scaled, y_train, X_test_scaled, y_test, epochs=100):\n",
    "    # Early stopping with optimized parameters\n",
    "    callbacks = [\n",
    "        EarlyStopping(\n",
    "            monitor='loss',\n",
    "            patience=10,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='loss',\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "            min_lr=1e-5,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # Train model with larger batch size for better speed\n",
    "    history = model.fit(\n",
    "        X_train_scaled, y_train,\n",
    "        epochs=epochs,\n",
    "        batch_size=128,\n",
    "        validation_split=0.0,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Evaluate model\n",
    "    y_pred = model.predict(X_test_scaled, batch_size=128, verbose=0)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    \n",
    "    return history, mse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "66054835",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing    import StandardScaler\n",
    "\n",
    "\n",
    "def prepare_data(option_only, save_paths=True):\n",
    "    # 1) Define features & targets\n",
    "    exclude = [\n",
    "        'iv_bs_error', 'iv_ahbs', 'iv_ahbs_error', 'iv_bs',\n",
    "        'iv_cw', 'iv_cw_error', 'impl_volatility', 'ID'\n",
    "    ]\n",
    "    features = joblib.load(r'C:\\Users\\maxva\\OneDrive - Tilburg University\\Msc. Data Science\\Master Thesis\\Code\\Models\\LigthGBM Forecast\\Firm Characteristics\\feature_columns.pkl')    \n",
    "\n",
    "    X_train = option_only[features]\n",
    "    y_train = {\n",
    "        'bs': option_only['iv_bs_error'],\n",
    "        'ahbs': option_only['iv_ahbs_error'],\n",
    "        'cw': option_only['iv_cw_error']\n",
    "    }\n",
    "\n",
    "    # 3) Scale training data\n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "    X_train_scaled = pd.DataFrame(\n",
    "        scaler.transform(X_train),\n",
    "        columns=features\n",
    "    )\n",
    "\n",
    "    # 4) Persist artifacts if required\n",
    "    if save_paths:\n",
    "        joblib.dump(scaler, 'scaler.pkl')\n",
    "        joblib.dump(features, 'feature_columns.pkl')\n",
    "\n",
    "    return X_train_scaled, y_train, scaler, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a205ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_add_to_test_data(models, test_data, feature_columns, scaler, error_type):\n",
    "\n",
    "    # Create a copy of the test data to avoid modifying the original\n",
    "    result_df = test_data\n",
    "   \n",
    "    # Extract features from test data as DataFrame (not as numpy array)\n",
    "    X_test = test_data[feature_columns]\n",
    "   \n",
    "    # Scale the features using the pre-fitted scaler and convert back to DataFrame\n",
    "    # This preserves feature names and prevents the warning\n",
    "    X_test_scaled = pd.DataFrame(\n",
    "        scaler.transform(X_test),\n",
    "        columns=feature_columns,\n",
    "        index=X_test.index\n",
    "    )\n",
    "   \n",
    "    # Original value column name\n",
    "    original_column = f'iv_{error_type}'\n",
    "   \n",
    "    # Generate predictions for each model\n",
    "    for model_name, model in models.items():\n",
    "        # Make predictions\n",
    "        predictions = model.predict(X_test_scaled, batch_size=128, verbose=0)\n",
    "       \n",
    "        # Flatten predictions if needed\n",
    "        if len(predictions.shape) > 1:\n",
    "            predictions = predictions.flatten()\n",
    "       \n",
    "        # Add predictions to the dataframe\n",
    "        column_name = f'iv_{error_type}_pred_{model_name}'\n",
    "        result_df[column_name] = predictions\n",
    "       \n",
    "        # Calculate corrected value by adding the error prediction to the original value\n",
    "        result_df[f'iv_{error_type}_corrected_{model_name}'] = result_df[original_column] + predictions\n",
    "   \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefb6278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training models for bs error correction ===\n",
      "\n",
      "Training NN3_bs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxva\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 769us/step - loss: 0.0027 - learning_rate: 0.0100\n",
      "Epoch 2/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 773us/step - loss: 0.0010 - learning_rate: 0.0100\n",
      "Epoch 3/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 794us/step - loss: 9.4150e-04 - learning_rate: 0.0100\n",
      "Epoch 4/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 770us/step - loss: 9.0280e-04 - learning_rate: 0.0100\n",
      "Epoch 5/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 774us/step - loss: 8.7318e-04 - learning_rate: 0.0100\n",
      "Epoch 6/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 773us/step - loss: 8.3908e-04 - learning_rate: 0.0100\n",
      "Epoch 7/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 801us/step - loss: 8.1906e-04 - learning_rate: 0.0100\n",
      "Epoch 8/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 747us/step - loss: 8.0558e-04 - learning_rate: 0.0100\n",
      "Epoch 9/50\n",
      "\u001b[1m16678/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 751us/step - loss: 7.9963e-04\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 752us/step - loss: 7.9963e-04 - learning_rate: 0.0100\n",
      "Epoch 10/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 776us/step - loss: 6.5753e-04 - learning_rate: 0.0050\n",
      "Epoch 11/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 775us/step - loss: 6.0814e-04 - learning_rate: 0.0050\n",
      "Epoch 12/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 757us/step - loss: 5.8816e-04 - learning_rate: 0.0050\n",
      "Epoch 13/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 774us/step - loss: 5.7420e-04 - learning_rate: 0.0050\n",
      "Epoch 14/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 769us/step - loss: 5.7380e-04 - learning_rate: 0.0050\n",
      "Epoch 15/50\n",
      "\u001b[1m16656/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 773us/step - loss: 5.6569e-04\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 774us/step - loss: 5.6568e-04 - learning_rate: 0.0050\n",
      "Epoch 16/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 763us/step - loss: 5.0210e-04 - learning_rate: 0.0025\n",
      "Epoch 17/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 795us/step - loss: 4.7737e-04 - learning_rate: 0.0025\n",
      "Epoch 18/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 775us/step - loss: 4.7012e-04 - learning_rate: 0.0025\n",
      "Epoch 19/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 767us/step - loss: 4.6344e-04 - learning_rate: 0.0025\n",
      "Epoch 20/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 755us/step - loss: 4.5945e-04 - learning_rate: 0.0025\n",
      "Epoch 21/50\n",
      "\u001b[1m16631/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 761us/step - loss: 4.5601e-04\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 762us/step - loss: 4.5602e-04 - learning_rate: 0.0025\n",
      "Epoch 22/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 763us/step - loss: 4.2736e-04 - learning_rate: 0.0012\n",
      "Epoch 23/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 770us/step - loss: 4.2013e-04 - learning_rate: 0.0012\n",
      "Epoch 24/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 764us/step - loss: 4.1630e-04 - learning_rate: 0.0012\n",
      "Epoch 25/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 790us/step - loss: 4.1344e-04 - learning_rate: 0.0012\n",
      "Epoch 26/50\n",
      "\u001b[1m16676/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 795us/step - loss: 4.1154e-04\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 796us/step - loss: 4.1154e-04 - learning_rate: 0.0012\n",
      "Epoch 27/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 776us/step - loss: 3.9583e-04 - learning_rate: 6.2500e-04\n",
      "Epoch 28/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 807us/step - loss: 3.9436e-04 - learning_rate: 6.2500e-04\n",
      "Epoch 29/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 788us/step - loss: 3.9288e-04 - learning_rate: 6.2500e-04\n",
      "Epoch 30/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 793us/step - loss: 3.9152e-04 - learning_rate: 6.2500e-04\n",
      "Epoch 31/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 786us/step - loss: 3.8905e-04 - learning_rate: 6.2500e-04\n",
      "Epoch 32/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 755us/step - loss: 3.8809e-04 - learning_rate: 6.2500e-04\n",
      "Epoch 33/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 804us/step - loss: 3.8714e-04 - learning_rate: 6.2500e-04\n",
      "Epoch 34/50\n",
      "\u001b[1m16653/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 766us/step - loss: 3.8877e-04\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 767us/step - loss: 3.8877e-04 - learning_rate: 6.2500e-04\n",
      "Epoch 35/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 765us/step - loss: 3.7965e-04 - learning_rate: 3.1250e-04\n",
      "Epoch 36/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 749us/step - loss: 3.8052e-04 - learning_rate: 3.1250e-04\n",
      "Epoch 37/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 792us/step - loss: 3.7739e-04 - learning_rate: 3.1250e-04\n",
      "Epoch 38/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 803us/step - loss: 3.7658e-04 - learning_rate: 3.1250e-04\n",
      "Epoch 39/50\n",
      "\u001b[1m16623/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 762us/step - loss: 3.7732e-04\n",
      "Epoch 39: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 763us/step - loss: 3.7732e-04 - learning_rate: 3.1250e-04\n",
      "Epoch 40/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 770us/step - loss: 3.7382e-04 - learning_rate: 1.5625e-04\n",
      "Epoch 41/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 783us/step - loss: 3.7177e-04 - learning_rate: 1.5625e-04\n",
      "Epoch 42/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 763us/step - loss: 3.7311e-04 - learning_rate: 1.5625e-04\n",
      "Epoch 43/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 796us/step - loss: 3.7103e-04 - learning_rate: 1.5625e-04\n",
      "Epoch 44/50\n",
      "\u001b[1m16645/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 793us/step - loss: 3.7189e-04\n",
      "Epoch 44: ReduceLROnPlateau reducing learning rate to 7.812499825377017e-05.\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 794us/step - loss: 3.7189e-04 - learning_rate: 1.5625e-04\n",
      "Epoch 45/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 767us/step - loss: 3.6954e-04 - learning_rate: 7.8125e-05\n",
      "Epoch 46/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 757us/step - loss: 3.7038e-04 - learning_rate: 7.8125e-05\n",
      "Epoch 47/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 754us/step - loss: 3.6849e-04 - learning_rate: 7.8125e-05\n",
      "Epoch 48/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 748us/step - loss: 3.6890e-04 - learning_rate: 7.8125e-05\n",
      "Epoch 49/50\n",
      "\u001b[1m16626/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 762us/step - loss: 3.6925e-04\n",
      "Epoch 49: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 763us/step - loss: 3.6926e-04 - learning_rate: 7.8125e-05\n",
      "Epoch 50/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 766us/step - loss: 3.7004e-04 - learning_rate: 3.9062e-05\n",
      "Restoring model weights from the end of the best epoch: 50.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN3_bs Test MSE: 0.0003963788125008712\n",
      "\n",
      "Training NN4_bs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxva\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 817us/step - loss: 0.0026 - learning_rate: 0.0100\n",
      "Epoch 2/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 828us/step - loss: 0.0010 - learning_rate: 0.0100\n",
      "Epoch 3/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 827us/step - loss: 9.5052e-04 - learning_rate: 0.0100\n",
      "Epoch 4/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 821us/step - loss: 8.9690e-04 - learning_rate: 0.0100\n",
      "Epoch 5/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 832us/step - loss: 8.8671e-04 - learning_rate: 0.0100\n",
      "Epoch 6/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 816us/step - loss: 8.5212e-04 - learning_rate: 0.0100\n",
      "Epoch 7/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 819us/step - loss: 8.4345e-04 - learning_rate: 0.0100\n",
      "Epoch 8/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 800us/step - loss: 8.1834e-04 - learning_rate: 0.0100\n",
      "Epoch 9/50\n",
      "\u001b[1m16664/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 806us/step - loss: 8.1161e-04\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 807us/step - loss: 8.1160e-04 - learning_rate: 0.0100\n",
      "Epoch 10/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 809us/step - loss: 6.7349e-04 - learning_rate: 0.0050\n",
      "Epoch 11/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 802us/step - loss: 6.2024e-04 - learning_rate: 0.0050\n",
      "Epoch 12/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 814us/step - loss: 5.9906e-04 - learning_rate: 0.0050\n",
      "Epoch 13/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 812us/step - loss: 5.9037e-04 - learning_rate: 0.0050\n",
      "Epoch 14/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 814us/step - loss: 5.8156e-04 - learning_rate: 0.0050\n",
      "Epoch 15/50\n",
      "\u001b[1m16633/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 806us/step - loss: 5.7330e-04\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 807us/step - loss: 5.7329e-04 - learning_rate: 0.0050\n",
      "Epoch 16/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 793us/step - loss: 5.1013e-04 - learning_rate: 0.0025\n",
      "Epoch 17/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 804us/step - loss: 4.8973e-04 - learning_rate: 0.0025\n",
      "Epoch 18/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 797us/step - loss: 4.8211e-04 - learning_rate: 0.0025\n",
      "Epoch 19/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 794us/step - loss: 4.7296e-04 - learning_rate: 0.0025\n",
      "Epoch 20/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 794us/step - loss: 4.7293e-04 - learning_rate: 0.0025\n",
      "Epoch 21/50\n",
      "\u001b[1m16683/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 799us/step - loss: 4.6765e-04\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 800us/step - loss: 4.6765e-04 - learning_rate: 0.0025\n",
      "Epoch 22/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 815us/step - loss: 4.3947e-04 - learning_rate: 0.0012\n",
      "Epoch 23/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 805us/step - loss: 4.3086e-04 - learning_rate: 0.0012\n",
      "Epoch 24/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 795us/step - loss: 4.2584e-04 - learning_rate: 0.0012\n",
      "Epoch 25/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 798us/step - loss: 4.2389e-04 - learning_rate: 0.0012\n",
      "Epoch 26/50\n",
      "\u001b[1m16684/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 807us/step - loss: 4.2455e-04\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 808us/step - loss: 4.2455e-04 - learning_rate: 0.0012\n",
      "Epoch 27/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 796us/step - loss: 4.0688e-04 - learning_rate: 6.2500e-04\n",
      "Epoch 28/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 775us/step - loss: 4.0310e-04 - learning_rate: 6.2500e-04\n",
      "Epoch 29/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 781us/step - loss: 4.0256e-04 - learning_rate: 6.2500e-04\n",
      "Epoch 30/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 781us/step - loss: 4.0272e-04 - learning_rate: 6.2500e-04\n",
      "Epoch 31/50\n",
      "\u001b[1m16664/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 781us/step - loss: 4.0314e-04\n",
      "Epoch 31: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 782us/step - loss: 4.0314e-04 - learning_rate: 6.2500e-04\n",
      "Epoch 32/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 779us/step - loss: 3.9366e-04 - learning_rate: 3.1250e-04\n",
      "Epoch 33/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 778us/step - loss: 3.9277e-04 - learning_rate: 3.1250e-04\n",
      "Epoch 34/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 765us/step - loss: 3.9128e-04 - learning_rate: 3.1250e-04\n",
      "Epoch 35/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 758us/step - loss: 3.9151e-04 - learning_rate: 3.1250e-04\n",
      "Epoch 36/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 756us/step - loss: 3.8956e-04 - learning_rate: 3.1250e-04\n",
      "Epoch 37/50\n",
      "\u001b[1m16651/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 760us/step - loss: 3.9075e-04\n",
      "Epoch 37: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 761us/step - loss: 3.9074e-04 - learning_rate: 3.1250e-04\n",
      "Epoch 38/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 748us/step - loss: 3.8683e-04 - learning_rate: 1.5625e-04\n",
      "Epoch 39/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 751us/step - loss: 3.8517e-04 - learning_rate: 1.5625e-04\n",
      "Epoch 40/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 753us/step - loss: 3.8472e-04 - learning_rate: 1.5625e-04\n",
      "Epoch 41/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 758us/step - loss: 3.8500e-04 - learning_rate: 1.5625e-04\n",
      "Epoch 42/50\n",
      "\u001b[1m16652/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 749us/step - loss: 3.8445e-04\n",
      "Epoch 42: ReduceLROnPlateau reducing learning rate to 7.812499825377017e-05.\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 750us/step - loss: 3.8445e-04 - learning_rate: 1.5625e-04\n",
      "Epoch 43/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 749us/step - loss: 3.8285e-04 - learning_rate: 7.8125e-05\n",
      "Epoch 44/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 755us/step - loss: 3.8332e-04 - learning_rate: 7.8125e-05\n",
      "Epoch 45/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 756us/step - loss: 3.8390e-04 - learning_rate: 7.8125e-05\n",
      "Epoch 46/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 756us/step - loss: 3.8144e-04 - learning_rate: 7.8125e-05\n",
      "Epoch 47/50\n",
      "\u001b[1m16628/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 750us/step - loss: 3.8097e-04\n",
      "Epoch 47: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 751us/step - loss: 3.8098e-04 - learning_rate: 7.8125e-05\n",
      "Epoch 48/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 759us/step - loss: 3.8149e-04 - learning_rate: 3.9062e-05\n",
      "Epoch 49/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 763us/step - loss: 3.8096e-04 - learning_rate: 3.9062e-05\n",
      "Epoch 50/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 755us/step - loss: 3.8131e-04 - learning_rate: 3.9062e-05\n",
      "Restoring model weights from the end of the best epoch: 50.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN4_bs Test MSE: 0.00040919232438068947\n",
      "\n",
      "=== Training models for ahbs error correction ===\n",
      "\n",
      "Training NN3_ahbs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxva\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 735us/step - loss: 0.0024 - learning_rate: 0.0100\n",
      "Epoch 2/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 729us/step - loss: 0.0010 - learning_rate: 0.0100\n",
      "Epoch 3/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 738us/step - loss: 9.2296e-04 - learning_rate: 0.0100\n",
      "Epoch 4/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 728us/step - loss: 9.0230e-04 - learning_rate: 0.0100\n",
      "Epoch 5/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 733us/step - loss: 8.6539e-04 - learning_rate: 0.0100\n",
      "Epoch 6/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 731us/step - loss: 8.5990e-04 - learning_rate: 0.0100\n",
      "Epoch 7/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 726us/step - loss: 8.5535e-04 - learning_rate: 0.0100\n",
      "Epoch 8/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 731us/step - loss: 8.3378e-04 - learning_rate: 0.0100\n",
      "Epoch 9/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 732us/step - loss: 8.2051e-04 - learning_rate: 0.0100\n",
      "Epoch 10/50\n",
      "\u001b[1m16663/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 736us/step - loss: 8.1858e-04\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 737us/step - loss: 8.1858e-04 - learning_rate: 0.0100\n",
      "Epoch 11/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 732us/step - loss: 7.0305e-04 - learning_rate: 0.0050\n",
      "Epoch 12/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 735us/step - loss: 6.4938e-04 - learning_rate: 0.0050\n",
      "Epoch 13/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 737us/step - loss: 6.2737e-04 - learning_rate: 0.0050\n",
      "Epoch 14/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 736us/step - loss: 6.1755e-04 - learning_rate: 0.0050\n",
      "Epoch 15/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 731us/step - loss: 6.0522e-04 - learning_rate: 0.0050\n",
      "Epoch 16/50\n",
      "\u001b[1m16676/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 733us/step - loss: 5.9746e-04\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 733us/step - loss: 5.9746e-04 - learning_rate: 0.0050\n",
      "Epoch 17/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 734us/step - loss: 5.4230e-04 - learning_rate: 0.0025\n",
      "Epoch 18/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 736us/step - loss: 5.2007e-04 - learning_rate: 0.0025\n",
      "Epoch 19/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 733us/step - loss: 5.1560e-04 - learning_rate: 0.0025\n",
      "Epoch 20/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 735us/step - loss: 5.1040e-04 - learning_rate: 0.0025\n",
      "Epoch 21/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 735us/step - loss: 5.0536e-04 - learning_rate: 0.0025\n",
      "Epoch 22/50\n",
      "\u001b[1m16631/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 734us/step - loss: 4.9976e-04\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 734us/step - loss: 4.9977e-04 - learning_rate: 0.0025\n",
      "Epoch 23/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 731us/step - loss: 4.7151e-04 - learning_rate: 0.0012\n",
      "Epoch 24/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 734us/step - loss: 4.6372e-04 - learning_rate: 0.0012\n",
      "Epoch 25/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 731us/step - loss: 4.6355e-04 - learning_rate: 0.0012\n",
      "Epoch 26/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 732us/step - loss: 4.5715e-04 - learning_rate: 0.0012\n",
      "Epoch 27/50\n",
      "\u001b[1m16622/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 736us/step - loss: 4.5440e-04\n",
      "Epoch 27: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 737us/step - loss: 4.5441e-04 - learning_rate: 0.0012\n",
      "Epoch 28/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 733us/step - loss: 4.4238e-04 - learning_rate: 6.2500e-04\n",
      "Epoch 29/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 731us/step - loss: 4.3857e-04 - learning_rate: 6.2500e-04\n",
      "Epoch 30/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 736us/step - loss: 4.3438e-04 - learning_rate: 6.2500e-04\n",
      "Epoch 31/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 735us/step - loss: 4.3867e-04 - learning_rate: 6.2500e-04\n",
      "Epoch 32/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 737us/step - loss: 4.3436e-04 - learning_rate: 6.2500e-04\n",
      "Epoch 33/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 738us/step - loss: 4.3460e-04 - learning_rate: 6.2500e-04\n",
      "Epoch 34/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 736us/step - loss: 4.3166e-04 - learning_rate: 6.2500e-04\n",
      "Epoch 35/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 731us/step - loss: 4.3243e-04 - learning_rate: 6.2500e-04\n",
      "Epoch 36/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 729us/step - loss: 4.2956e-04 - learning_rate: 6.2500e-04\n",
      "Epoch 37/50\n",
      "\u001b[1m16660/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 736us/step - loss: 4.2927e-04\n",
      "Epoch 37: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 737us/step - loss: 4.2928e-04 - learning_rate: 6.2500e-04\n",
      "Epoch 38/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 732us/step - loss: 4.2354e-04 - learning_rate: 3.1250e-04\n",
      "Epoch 39/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 735us/step - loss: 4.2154e-04 - learning_rate: 3.1250e-04\n",
      "Epoch 40/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 735us/step - loss: 4.2097e-04 - learning_rate: 3.1250e-04\n",
      "Epoch 41/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 734us/step - loss: 4.2084e-04 - learning_rate: 3.1250e-04\n",
      "Epoch 42/50\n",
      "\u001b[1m16666/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 739us/step - loss: 4.1994e-04\n",
      "Epoch 42: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 740us/step - loss: 4.1994e-04 - learning_rate: 3.1250e-04\n",
      "Epoch 43/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 733us/step - loss: 4.1818e-04 - learning_rate: 1.5625e-04\n",
      "Epoch 44/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 735us/step - loss: 4.1618e-04 - learning_rate: 1.5625e-04\n",
      "Epoch 45/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 739us/step - loss: 4.1745e-04 - learning_rate: 1.5625e-04\n",
      "Epoch 46/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 737us/step - loss: 4.1700e-04 - learning_rate: 1.5625e-04\n",
      "Epoch 47/50\n",
      "\u001b[1m16646/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 739us/step - loss: 4.1511e-04\n",
      "Epoch 47: ReduceLROnPlateau reducing learning rate to 7.812499825377017e-05.\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 740us/step - loss: 4.1511e-04 - learning_rate: 1.5625e-04\n",
      "Epoch 48/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 735us/step - loss: 4.1483e-04 - learning_rate: 7.8125e-05\n",
      "Epoch 49/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 747us/step - loss: 4.1378e-04 - learning_rate: 7.8125e-05\n",
      "Epoch 50/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 746us/step - loss: 4.1373e-04 - learning_rate: 7.8125e-05\n",
      "Restoring model weights from the end of the best epoch: 50.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN3_ahbs Test MSE: 0.00044536452411562976\n",
      "\n",
      "Training NN4_ahbs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxva\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 770us/step - loss: 0.0035 - learning_rate: 0.0100\n",
      "Epoch 2/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 777us/step - loss: 0.0010 - learning_rate: 0.0100\n",
      "Epoch 3/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 769us/step - loss: 9.4990e-04 - learning_rate: 0.0100\n",
      "Epoch 4/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 774us/step - loss: 9.1521e-04 - learning_rate: 0.0100\n",
      "Epoch 5/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 777us/step - loss: 8.7676e-04 - learning_rate: 0.0100\n",
      "Epoch 6/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 779us/step - loss: 8.5132e-04 - learning_rate: 0.0100\n",
      "Epoch 7/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 768us/step - loss: 8.3809e-04 - learning_rate: 0.0100\n",
      "Epoch 8/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 779us/step - loss: 8.2476e-04 - learning_rate: 0.0100\n",
      "Epoch 9/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 770us/step - loss: 8.1408e-04 - learning_rate: 0.0100\n",
      "Epoch 10/50\n",
      "\u001b[1m16654/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 778us/step - loss: 8.0794e-04\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 779us/step - loss: 8.0793e-04 - learning_rate: 0.0100\n",
      "Epoch 11/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 774us/step - loss: 6.8793e-04 - learning_rate: 0.0050\n",
      "Epoch 12/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 768us/step - loss: 6.3810e-04 - learning_rate: 0.0050\n",
      "Epoch 13/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 773us/step - loss: 6.2117e-04 - learning_rate: 0.0050\n",
      "Epoch 14/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 773us/step - loss: 6.1339e-04 - learning_rate: 0.0050\n",
      "Epoch 15/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 770us/step - loss: 5.9799e-04 - learning_rate: 0.0050\n",
      "Epoch 16/50\n",
      "\u001b[1m16669/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 777us/step - loss: 5.9326e-04\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 778us/step - loss: 5.9327e-04 - learning_rate: 0.0050\n",
      "Epoch 17/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 775us/step - loss: 5.4377e-04 - learning_rate: 0.0025\n",
      "Epoch 18/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 766us/step - loss: 5.2433e-04 - learning_rate: 0.0025\n",
      "Epoch 19/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 773us/step - loss: 5.1364e-04 - learning_rate: 0.0025\n",
      "Epoch 20/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 773us/step - loss: 5.0844e-04 - learning_rate: 0.0025\n",
      "Epoch 21/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 776us/step - loss: 5.0426e-04 - learning_rate: 0.0025\n",
      "Epoch 22/50\n",
      "\u001b[1m16615/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 768us/step - loss: 4.9902e-04\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 769us/step - loss: 4.9903e-04 - learning_rate: 0.0025\n",
      "Epoch 23/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 776us/step - loss: 4.7301e-04 - learning_rate: 0.0012\n",
      "Epoch 24/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 776us/step - loss: 4.6649e-04 - learning_rate: 0.0012\n",
      "Epoch 25/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 772us/step - loss: 4.6368e-04 - learning_rate: 0.0012\n",
      "Epoch 26/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 775us/step - loss: 4.5797e-04 - learning_rate: 0.0012\n",
      "Epoch 27/50\n",
      "\u001b[1m16652/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 772us/step - loss: 4.5796e-04\n",
      "Epoch 27: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 773us/step - loss: 4.5796e-04 - learning_rate: 0.0012\n",
      "Epoch 28/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 768us/step - loss: 4.4628e-04 - learning_rate: 6.2500e-04\n",
      "Epoch 29/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 774us/step - loss: 4.4290e-04 - learning_rate: 6.2500e-04\n",
      "Epoch 30/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 782us/step - loss: 4.4079e-04 - learning_rate: 6.2500e-04\n",
      "Epoch 31/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 773us/step - loss: 4.4180e-04 - learning_rate: 6.2500e-04\n",
      "Epoch 32/50\n",
      "\u001b[1m16665/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 774us/step - loss: 4.3995e-04\n",
      "Epoch 32: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 775us/step - loss: 4.3995e-04 - learning_rate: 6.2500e-04\n",
      "Epoch 33/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 780us/step - loss: 4.3161e-04 - learning_rate: 3.1250e-04\n",
      "Epoch 34/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 775us/step - loss: 4.3064e-04 - learning_rate: 3.1250e-04\n",
      "Epoch 35/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 768us/step - loss: 4.3066e-04 - learning_rate: 3.1250e-04\n",
      "Epoch 36/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 779us/step - loss: 4.2853e-04 - learning_rate: 3.1250e-04\n",
      "Epoch 37/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 775us/step - loss: 4.2908e-04 - learning_rate: 3.1250e-04\n",
      "Epoch 38/50\n",
      "\u001b[1m16636/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 777us/step - loss: 4.2870e-04\n",
      "Epoch 38: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 778us/step - loss: 4.2871e-04 - learning_rate: 3.1250e-04\n",
      "Epoch 39/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 768us/step - loss: 4.2587e-04 - learning_rate: 1.5625e-04\n",
      "Epoch 40/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 775us/step - loss: 4.2654e-04 - learning_rate: 1.5625e-04\n",
      "Epoch 41/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 767us/step - loss: 4.2545e-04 - learning_rate: 1.5625e-04\n",
      "Epoch 42/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 775us/step - loss: 4.2465e-04 - learning_rate: 1.5625e-04\n",
      "Epoch 43/50\n",
      "\u001b[1m16634/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 775us/step - loss: 4.2453e-04\n",
      "Epoch 43: ReduceLROnPlateau reducing learning rate to 7.812499825377017e-05.\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 776us/step - loss: 4.2454e-04 - learning_rate: 1.5625e-04\n",
      "Epoch 44/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 775us/step - loss: 4.2510e-04 - learning_rate: 7.8125e-05\n",
      "Epoch 45/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 780us/step - loss: 4.2256e-04 - learning_rate: 7.8125e-05\n",
      "Epoch 46/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 775us/step - loss: 4.2027e-04 - learning_rate: 7.8125e-05\n",
      "Epoch 47/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 785us/step - loss: 4.2247e-04 - learning_rate: 7.8125e-05\n",
      "Epoch 48/50\n",
      "\u001b[1m16626/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 773us/step - loss: 4.2326e-04\n",
      "Epoch 48: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 774us/step - loss: 4.2325e-04 - learning_rate: 7.8125e-05\n",
      "Epoch 49/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 774us/step - loss: 4.2327e-04 - learning_rate: 3.9062e-05\n",
      "Epoch 50/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 777us/step - loss: 4.2091e-04 - learning_rate: 3.9062e-05\n",
      "Restoring model weights from the end of the best epoch: 50.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN4_ahbs Test MSE: 0.0004559696123782911\n",
      "\n",
      "=== Training models for cw error correction ===\n",
      "\n",
      "Training NN3_cw...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxva\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 791us/step - loss: 0.0017 - learning_rate: 0.0100\n",
      "Epoch 2/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 791us/step - loss: 9.0532e-04 - learning_rate: 0.0100\n",
      "Epoch 3/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 791us/step - loss: 8.3850e-04 - learning_rate: 0.0100\n",
      "Epoch 4/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 764us/step - loss: 8.0183e-04 - learning_rate: 0.0100\n",
      "Epoch 5/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 767us/step - loss: 7.9543e-04 - learning_rate: 0.0100\n",
      "Epoch 6/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 781us/step - loss: 7.6372e-04 - learning_rate: 0.0100\n",
      "Epoch 7/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 782us/step - loss: 7.4990e-04 - learning_rate: 0.0100\n",
      "Epoch 8/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 778us/step - loss: 7.4120e-04 - learning_rate: 0.0100\n",
      "Epoch 9/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 782us/step - loss: 7.2362e-04 - learning_rate: 0.0100\n",
      "Epoch 10/50\n",
      "\u001b[1m16628/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 810us/step - loss: 7.1396e-04\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 811us/step - loss: 7.1398e-04 - learning_rate: 0.0100\n",
      "Epoch 11/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 763us/step - loss: 6.0848e-04 - learning_rate: 0.0050\n",
      "Epoch 12/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 759us/step - loss: 5.6970e-04 - learning_rate: 0.0050\n",
      "Epoch 13/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 774us/step - loss: 5.5958e-04 - learning_rate: 0.0050\n",
      "Epoch 14/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 786us/step - loss: 5.4508e-04 - learning_rate: 0.0050\n",
      "Epoch 15/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 776us/step - loss: 5.3797e-04 - learning_rate: 0.0050\n",
      "Epoch 16/50\n",
      "\u001b[1m16655/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 781us/step - loss: 5.3127e-04\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 782us/step - loss: 5.3127e-04 - learning_rate: 0.0050\n",
      "Epoch 17/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 788us/step - loss: 4.7960e-04 - learning_rate: 0.0025\n",
      "Epoch 18/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 779us/step - loss: 4.6730e-04 - learning_rate: 0.0025\n",
      "Epoch 19/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 782us/step - loss: 4.6271e-04 - learning_rate: 0.0025\n",
      "Epoch 20/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 784us/step - loss: 4.5694e-04 - learning_rate: 0.0025\n",
      "Epoch 21/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 754us/step - loss: 4.5538e-04 - learning_rate: 0.0025\n",
      "Epoch 22/50\n",
      "\u001b[1m16658/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 753us/step - loss: 4.5077e-04\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 754us/step - loss: 4.5078e-04 - learning_rate: 0.0025\n",
      "Epoch 23/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 787us/step - loss: 4.3087e-04 - learning_rate: 0.0012\n",
      "Epoch 24/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 790us/step - loss: 4.2170e-04 - learning_rate: 0.0012\n",
      "Epoch 25/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 785us/step - loss: 4.1866e-04 - learning_rate: 0.0012\n",
      "Epoch 26/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 793us/step - loss: 4.1758e-04 - learning_rate: 0.0012\n",
      "Epoch 27/50\n",
      "\u001b[1m16672/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 803us/step - loss: 4.1745e-04\n",
      "Epoch 27: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 804us/step - loss: 4.1745e-04 - learning_rate: 0.0012\n",
      "Epoch 28/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 801us/step - loss: 4.0512e-04 - learning_rate: 6.2500e-04\n",
      "Epoch 29/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 799us/step - loss: 4.0206e-04 - learning_rate: 6.2500e-04\n",
      "Epoch 30/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 790us/step - loss: 4.0047e-04 - learning_rate: 6.2500e-04\n",
      "Epoch 31/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 831us/step - loss: 3.9987e-04 - learning_rate: 6.2500e-04\n",
      "Epoch 32/50\n",
      "\u001b[1m16645/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 812us/step - loss: 4.0060e-04\n",
      "Epoch 32: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 813us/step - loss: 4.0060e-04 - learning_rate: 6.2500e-04\n",
      "Epoch 33/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 818us/step - loss: 3.9399e-04 - learning_rate: 3.1250e-04\n",
      "Epoch 34/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 779us/step - loss: 3.9187e-04 - learning_rate: 3.1250e-04\n",
      "Epoch 35/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 773us/step - loss: 3.9184e-04 - learning_rate: 3.1250e-04\n",
      "Epoch 36/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 801us/step - loss: 3.9295e-04 - learning_rate: 3.1250e-04\n",
      "Epoch 37/50\n",
      "\u001b[1m16647/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 783us/step - loss: 3.9053e-04\n",
      "Epoch 37: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 784us/step - loss: 3.9054e-04 - learning_rate: 3.1250e-04\n",
      "Epoch 38/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 801us/step - loss: 3.8848e-04 - learning_rate: 1.5625e-04\n",
      "Epoch 39/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 762us/step - loss: 3.8843e-04 - learning_rate: 1.5625e-04\n",
      "Epoch 40/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 747us/step - loss: 3.8858e-04 - learning_rate: 1.5625e-04\n",
      "Epoch 41/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 764us/step - loss: 3.8744e-04 - learning_rate: 1.5625e-04\n",
      "Epoch 42/50\n",
      "\u001b[1m16648/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 760us/step - loss: 3.8767e-04\n",
      "Epoch 42: ReduceLROnPlateau reducing learning rate to 7.812499825377017e-05.\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 760us/step - loss: 3.8767e-04 - learning_rate: 1.5625e-04\n",
      "Epoch 43/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 760us/step - loss: 3.8671e-04 - learning_rate: 7.8125e-05\n",
      "Epoch 44/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 768us/step - loss: 3.8436e-04 - learning_rate: 7.8125e-05\n",
      "Epoch 45/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 793us/step - loss: 3.8429e-04 - learning_rate: 7.8125e-05\n",
      "Epoch 46/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 786us/step - loss: 3.8632e-04 - learning_rate: 7.8125e-05\n",
      "Epoch 47/50\n",
      "\u001b[1m16672/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 795us/step - loss: 3.8640e-04\n",
      "Epoch 47: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 796us/step - loss: 3.8640e-04 - learning_rate: 7.8125e-05\n",
      "Epoch 48/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 798us/step - loss: 3.8552e-04 - learning_rate: 3.9062e-05\n",
      "Epoch 49/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 809us/step - loss: 3.8500e-04 - learning_rate: 3.9062e-05\n",
      "Epoch 50/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 798us/step - loss: 3.8344e-04 - learning_rate: 3.9062e-05\n",
      "Restoring model weights from the end of the best epoch: 50.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN3_cw Test MSE: 0.0004091520450459662\n",
      "\n",
      "Training NN4_cw...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxva\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 830us/step - loss: 0.0018 - learning_rate: 0.0100\n",
      "Epoch 2/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 841us/step - loss: 9.1130e-04 - learning_rate: 0.0100\n",
      "Epoch 3/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 837us/step - loss: 8.7478e-04 - learning_rate: 0.0100\n",
      "Epoch 4/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 834us/step - loss: 8.1453e-04 - learning_rate: 0.0100\n",
      "Epoch 5/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 832us/step - loss: 7.9200e-04 - learning_rate: 0.0100\n",
      "Epoch 6/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 831us/step - loss: 7.6072e-04 - learning_rate: 0.0100\n",
      "Epoch 7/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 832us/step - loss: 7.5602e-04 - learning_rate: 0.0100\n",
      "Epoch 8/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 838us/step - loss: 7.3736e-04 - learning_rate: 0.0100\n",
      "Epoch 9/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 833us/step - loss: 7.3252e-04 - learning_rate: 0.0100\n",
      "Epoch 10/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 798us/step - loss: 7.2611e-04 - learning_rate: 0.0100\n",
      "Epoch 11/50\n",
      "\u001b[1m16660/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 788us/step - loss: 7.2010e-04\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 789us/step - loss: 7.2010e-04 - learning_rate: 0.0100\n",
      "Epoch 12/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 844us/step - loss: 6.1835e-04 - learning_rate: 0.0050\n",
      "Epoch 13/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 841us/step - loss: 5.6724e-04 - learning_rate: 0.0050\n",
      "Epoch 14/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 844us/step - loss: 5.5874e-04 - learning_rate: 0.0050\n",
      "Epoch 15/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 837us/step - loss: 5.4290e-04 - learning_rate: 0.0050\n",
      "Epoch 16/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 833us/step - loss: 5.3609e-04 - learning_rate: 0.0050\n",
      "Epoch 17/50\n",
      "\u001b[1m16661/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 834us/step - loss: 5.3029e-04\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 835us/step - loss: 5.3029e-04 - learning_rate: 0.0050\n",
      "Epoch 18/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 837us/step - loss: 4.8259e-04 - learning_rate: 0.0025\n",
      "Epoch 19/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 834us/step - loss: 4.6836e-04 - learning_rate: 0.0025\n",
      "Epoch 20/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 836us/step - loss: 4.6306e-04 - learning_rate: 0.0025\n",
      "Epoch 21/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 836us/step - loss: 4.5737e-04 - learning_rate: 0.0025\n",
      "Epoch 22/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 818us/step - loss: 4.5691e-04 - learning_rate: 0.0025\n",
      "Epoch 23/50\n",
      "\u001b[1m16657/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 847us/step - loss: 4.4967e-04\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 848us/step - loss: 4.4967e-04 - learning_rate: 0.0025\n",
      "Epoch 24/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 847us/step - loss: 4.2734e-04 - learning_rate: 0.0012\n",
      "Epoch 25/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 847us/step - loss: 4.2242e-04 - learning_rate: 0.0012\n",
      "Epoch 26/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 818us/step - loss: 4.2133e-04 - learning_rate: 0.0012\n",
      "Epoch 27/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 806us/step - loss: 4.1705e-04 - learning_rate: 0.0012\n",
      "Epoch 28/50\n",
      "\u001b[1m16666/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 824us/step - loss: 4.1510e-04\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 825us/step - loss: 4.1511e-04 - learning_rate: 0.0012\n",
      "Epoch 29/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 806us/step - loss: 4.0720e-04 - learning_rate: 6.2500e-04\n",
      "Epoch 30/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 775us/step - loss: 4.0251e-04 - learning_rate: 6.2500e-04\n",
      "Epoch 31/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 820us/step - loss: 4.0273e-04 - learning_rate: 6.2500e-04\n",
      "Epoch 32/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 823us/step - loss: 4.0104e-04 - learning_rate: 6.2500e-04\n",
      "Epoch 33/50\n",
      "\u001b[1m16639/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 833us/step - loss: 3.9941e-04\n",
      "Epoch 33: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 834us/step - loss: 3.9941e-04 - learning_rate: 6.2500e-04\n",
      "Epoch 34/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 810us/step - loss: 3.9551e-04 - learning_rate: 3.1250e-04\n",
      "Epoch 35/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 814us/step - loss: 3.9511e-04 - learning_rate: 3.1250e-04\n",
      "Epoch 36/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 757us/step - loss: 3.9255e-04 - learning_rate: 3.1250e-04\n",
      "Epoch 37/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 813us/step - loss: 3.9227e-04 - learning_rate: 3.1250e-04\n",
      "Epoch 38/50\n",
      "\u001b[1m16636/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 811us/step - loss: 3.9078e-04\n",
      "Epoch 38: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 812us/step - loss: 3.9079e-04 - learning_rate: 3.1250e-04\n",
      "Epoch 39/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 845us/step - loss: 3.8902e-04 - learning_rate: 1.5625e-04\n",
      "Epoch 40/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 841us/step - loss: 3.8920e-04 - learning_rate: 1.5625e-04\n",
      "Epoch 41/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 829us/step - loss: 3.8821e-04 - learning_rate: 1.5625e-04\n",
      "Epoch 42/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 836us/step - loss: 3.8869e-04 - learning_rate: 1.5625e-04\n",
      "Epoch 43/50\n",
      "\u001b[1m16679/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 831us/step - loss: 3.8994e-04\n",
      "Epoch 43: ReduceLROnPlateau reducing learning rate to 7.812499825377017e-05.\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 832us/step - loss: 3.8994e-04 - learning_rate: 1.5625e-04\n",
      "Epoch 44/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 825us/step - loss: 3.8632e-04 - learning_rate: 7.8125e-05\n",
      "Epoch 45/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 833us/step - loss: 3.8732e-04 - learning_rate: 7.8125e-05\n",
      "Epoch 46/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 828us/step - loss: 3.8554e-04 - learning_rate: 7.8125e-05\n",
      "Epoch 47/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 840us/step - loss: 3.8732e-04 - learning_rate: 7.8125e-05\n",
      "Epoch 48/50\n",
      "\u001b[1m16648/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 838us/step - loss: 3.8669e-04\n",
      "Epoch 48: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 839us/step - loss: 3.8669e-04 - learning_rate: 7.8125e-05\n",
      "Epoch 49/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 829us/step - loss: 3.8482e-04 - learning_rate: 3.9062e-05\n",
      "Epoch 50/50\n",
      "\u001b[1m16686/16686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 830us/step - loss: 3.8727e-04 - learning_rate: 3.9062e-05\n",
      "Restoring model weights from the end of the best epoch: 50.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN4_cw Test MSE: 0.000411210025291508\n"
     ]
    }
   ],
   "source": [
    "\n",
    "###########################################\n",
    "# PART 4: COMPLETE WORKFLOW\n",
    "###########################################\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Step 2: Prepare data\n",
    "    X_train_scaled, y_train, scaler, feature_columns = prepare_data(train_data)\n",
    "    \n",
    "    # Step 3: Train models for each target variable\n",
    "    models = {}\n",
    "    results = {}\n",
    "    \n",
    "    # Dictionary to store all models\n",
    "    all_models = {\n",
    "        'bs': {},\n",
    "        'ahbs': {},\n",
    "        'cw': {}\n",
    "    }\n",
    "    \n",
    "    # Train models for each error type\n",
    "    for error_type in ['bs', 'ahbs', 'cw']:\n",
    "        print(f\"\\n=== Training models for {error_type} error correction ===\")\n",
    "\n",
    "        # Train each architecture\n",
    "        for nn_type in ['NN3', 'NN4']:\n",
    "            model_name = f\"{nn_type}_{error_type}\"\n",
    "            print(f\"\\nTraining {model_name}...\")\n",
    "\n",
    "            # Create model\n",
    "            model = create_nn_model(nn_type, X_train_scaled.shape[1])\n",
    "\n",
    "            # Train and evaluate model\n",
    "            history = train_and_evaluate_model(\n",
    "                model, X_train_scaled, y_train[error_type], epochs=100\n",
    "            )\n",
    "\n",
    "            # Store model and results\n",
    "            all_models[error_type][nn_type] = model\n",
    "            results[model_name] = {\n",
    "                'history': history\n",
    "            }\n",
    "\n",
    "            # Save model\n",
    "            save_model(model, f\"{model_name}_model_forecast.h5\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783a074b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading bs NN3...\n",
      "✓ Model loaded successfully with 4 layers\n",
      "Loading bs NN4...\n",
      "✓ Model loaded successfully with 5 layers\n",
      "Loading ahbs NN3...\n",
      "✓ Model loaded successfully with 4 layers\n",
      "Loading ahbs NN4...\n",
      "✓ Model loaded successfully with 5 layers\n",
      "Loading cw NN3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model loaded successfully with 4 layers\n",
      "Loading cw NN4...\n",
      "✓ Model loaded successfully with 5 layers\n",
      "\n",
      "=== Model Loading Summary ===\n",
      "bs NN3: ✓ Loaded\n",
      "bs NN4: ✓ Loaded\n",
      "ahbs NN3: ✓ Loaded\n",
      "ahbs NN4: ✓ Loaded\n",
      "cw NN3: ✓ Loaded\n",
      "cw NN4: ✓ Loaded\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def load_models(model_paths_dict):\n",
    "\n",
    "    loaded_models = {}\n",
    "    \n",
    "    for error_type in model_paths_dict:\n",
    "        loaded_models[error_type] = {}\n",
    "        \n",
    "        for model_name, path in model_paths_dict[error_type].items():\n",
    "            try:\n",
    "                # Handle Windows paths by using raw strings\n",
    "                path = path.replace('\\\\', '/')  # Convert backslashes to forward slashes\n",
    "                \n",
    "                # Check if file exists\n",
    "                if not os.path.exists(path):\n",
    "                    print(f\"✗ {error_type} {model_name}: File not found at {path}\")\n",
    "                    loaded_models[error_type][model_name] = None\n",
    "                    continue\n",
    "                \n",
    "                # Load the model\n",
    "                print(f\"Loading {error_type} {model_name}...\")\n",
    "                model = load_model(path)\n",
    "                \n",
    "                # Simple check: can we access layers?\n",
    "                num_layers = len(model.layers)\n",
    "                print(f\"✓ Model loaded successfully with {num_layers} layers\")\n",
    "                \n",
    "                # Store loaded model\n",
    "                loaded_models[error_type][model_name] = model\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"✗ Failed to load {error_type} {model_name}: {str(e)}\")\n",
    "                loaded_models[error_type][model_name] = None\n",
    "    \n",
    "    return loaded_models\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Model paths dictionary with proper path handling for Windows\n",
    "    # Use forward slashes or raw strings to avoid Unicode escape errors\n",
    "    model_paths_dict = {\n",
    "        'bs': {\n",
    "            'NN3': r\"C:/Users/maxva/OneDrive - Tilburg University/Msc. Data Science/Master Thesis/Code/Models/Neural Network forecast/Firm_characteristics/h1/NN3_bs_model_forecast.h5\",\n",
    "            'NN4': r\"C:/Users/maxva/OneDrive - Tilburg University/Msc. Data Science/Master Thesis/Code/Models/Neural Network forecast/Firm_characteristics/h1/NN4_bs_model_forecast.h5\"\n",
    "        },\n",
    "        'ahbs': {\n",
    "            'NN3': r\"C:/Users/maxva/OneDrive - Tilburg University/Msc. Data Science/Master Thesis/Code/Models/Neural Network forecast/Firm_characteristics/h1/NN3_ahbs_model_forecast.h5\",\n",
    "            'NN4': r\"C:/Users/maxva/OneDrive - Tilburg University/Msc. Data Science/Master Thesis/Code/Models/Neural Network forecast/Firm_characteristics/h1/NN4_ahbs_model_forecast.h5\"\n",
    "        },\n",
    "        'cw': {\n",
    "            'NN3': r\"C:/Users/maxva/OneDrive - Tilburg University/Msc. Data Science/Master Thesis/Code/Models/Neural Network forecast/Firm_characteristics/h1/NN3_cw_model_forecast.h5\",\n",
    "            'NN4': r\"C:/Users/maxva/OneDrive - Tilburg University/Msc. Data Science/Master Thesis/Code/Models/Neural Network forecast/Firm_characteristics/h1/NN4_cw_model_forecast.h5\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Load all models\n",
    "    loaded_models = load_models(model_paths_dict)\n",
    "    \n",
    "    # Print summary of loaded models\n",
    "    print(\"\\n=== Model Loading Summary ===\")\n",
    "    for error_type in loaded_models:\n",
    "        for model_name in loaded_models[error_type]:\n",
    "            status = \"✓ Loaded\" if loaded_models[error_type][model_name] is not None else \"✗ Failed\"\n",
    "            print(f\"{error_type} {model_name}: {status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7a3210f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Horizon 1 ===\n",
      "\n",
      "-- predicting corrections for bs --\n",
      "   impl_volatility     iv_bs  iv_bs_pred_NN3  iv_bs_corrected_NN3  \\\n",
      "0         0.201081  0.288808       -0.071727             0.217081   \n",
      "1         0.196421  0.288808       -0.068981             0.219827   \n",
      "2         0.191262  0.288808       -0.067503             0.221304   \n",
      "\n",
      "   iv_bs_pred_NN4  iv_bs_corrected_NN4  \n",
      "0       -0.066435             0.222373  \n",
      "1       -0.068248             0.220559  \n",
      "2       -0.069829             0.218979  \n",
      "\n",
      "-- predicting corrections for ahbs --\n",
      "   impl_volatility   iv_ahbs  iv_ahbs_pred_NN3  iv_ahbs_corrected_NN3  \\\n",
      "0         0.201081  0.299345         -0.060010               0.239336   \n",
      "1         0.196421  0.298947         -0.053910               0.245037   \n",
      "2         0.191262  0.298851         -0.053335               0.245516   \n",
      "\n",
      "   iv_ahbs_pred_NN4  iv_ahbs_corrected_NN4  \n",
      "0         -0.060623               0.238722  \n",
      "1         -0.060746               0.238201  \n",
      "2         -0.063248               0.235603  \n",
      "\n",
      "-- predicting corrections for cw --\n",
      "   impl_volatility     iv_cw  iv_cw_pred_NN3  iv_cw_corrected_NN3  \\\n",
      "0         0.201081  0.264812       -0.042575             0.222237   \n",
      "1         0.196421  0.266365       -0.043409             0.222956   \n",
      "2         0.191262  0.269537       -0.046371             0.223166   \n",
      "\n",
      "   iv_cw_pred_NN4  iv_cw_corrected_NN4  \n",
      "0       -0.026973             0.237839  \n",
      "1       -0.024944             0.241421  \n",
      "2       -0.026359             0.243179  \n",
      "\n",
      "-- test_data_h1 updated --\n",
      "\n",
      "=== Horizon 5 ===\n",
      "\n",
      "-- predicting corrections for bs --\n",
      "   impl_volatility     iv_bs  iv_bs_pred_NN3  iv_bs_corrected_NN3  \\\n",
      "0         0.246879  0.288808       -0.046322             0.242486   \n",
      "1         0.243913  0.288808       -0.047021             0.241787   \n",
      "2         0.239469  0.288808       -0.045688             0.243120   \n",
      "\n",
      "   iv_bs_pred_NN4  iv_bs_corrected_NN4  \n",
      "0       -0.055571             0.233236  \n",
      "1       -0.057025             0.231783  \n",
      "2       -0.059637             0.229170  \n",
      "\n",
      "-- predicting corrections for ahbs --\n",
      "   impl_volatility   iv_ahbs  iv_ahbs_pred_NN3  iv_ahbs_corrected_NN3  \\\n",
      "0         0.246879  0.296150         -0.051625               0.244524   \n",
      "1         0.243913  0.295814         -0.044618               0.251196   \n",
      "2         0.239469  0.295779         -0.043141               0.252639   \n",
      "\n",
      "   iv_ahbs_pred_NN4  iv_ahbs_corrected_NN4  \n",
      "0         -0.049928               0.246222  \n",
      "1         -0.049935               0.245879  \n",
      "2         -0.053297               0.242483  \n",
      "\n",
      "-- predicting corrections for cw --\n",
      "   impl_volatility     iv_cw  iv_cw_pred_NN3  iv_cw_corrected_NN3  \\\n",
      "0         0.246879  0.263030       -0.031937             0.231093   \n",
      "1         0.243913  0.264520       -0.034016             0.230504   \n",
      "2         0.239469  0.267565       -0.038106             0.229459   \n",
      "\n",
      "   iv_cw_pred_NN4  iv_cw_corrected_NN4  \n",
      "0       -0.033917             0.229113  \n",
      "1       -0.030707             0.233813  \n",
      "2       -0.029912             0.237653  \n",
      "\n",
      "-- test_data_h5 updated --\n",
      "\n",
      "=== Horizon 21 ===\n",
      "\n",
      "-- predicting corrections for bs --\n",
      "   impl_volatility     iv_bs  iv_bs_pred_NN3  iv_bs_corrected_NN3  \\\n",
      "0         0.159662  0.288808       -0.080497             0.208310   \n",
      "1         0.153761  0.288808       -0.084230             0.204577   \n",
      "2         0.146285  0.288808       -0.075737             0.213071   \n",
      "\n",
      "   iv_bs_pred_NN4  iv_bs_corrected_NN4  \n",
      "0       -0.075158             0.213649  \n",
      "1       -0.076602             0.212205  \n",
      "2       -0.062676             0.226131  \n",
      "\n",
      "-- predicting corrections for ahbs --\n",
      "   impl_volatility   iv_ahbs  iv_ahbs_pred_NN3  iv_ahbs_corrected_NN3  \\\n",
      "0         0.159662  0.274520         -0.054615               0.219905   \n",
      "1         0.153761  0.283228         -0.063658               0.219570   \n",
      "2         0.146285  0.296779         -0.072954               0.223825   \n",
      "\n",
      "   iv_ahbs_pred_NN4  iv_ahbs_corrected_NN4  \n",
      "0         -0.055302               0.219217  \n",
      "1         -0.059432               0.223796  \n",
      "2         -0.062343               0.234436  \n",
      "\n",
      "-- predicting corrections for cw --\n",
      "   impl_volatility     iv_cw  iv_cw_pred_NN3  iv_cw_corrected_NN3  \\\n",
      "0         0.159662  0.261731       -0.034262             0.227469   \n",
      "1         0.153761  0.294981       -0.073262             0.221719   \n",
      "2         0.146285  0.342147       -0.093937             0.248209   \n",
      "\n",
      "   iv_cw_pred_NN4  iv_cw_corrected_NN4  \n",
      "0       -0.043068             0.218663  \n",
      "1       -0.057940             0.237041  \n",
      "2       -0.091545             0.250602  \n",
      "\n",
      "-- test_data_h21 updated --\n"
     ]
    }
   ],
   "source": [
    "\n",
    "scaler_path = r\"C:\\Users\\maxva\\OneDrive - Tilburg University\\Msc. Data Science\\Master Thesis\\Code\\Models\\Neural Network forecast\\Firm_characteristics\\h1\\scaler.pkl\"\n",
    "scaler = joblib.load(scaler_path)\n",
    "\n",
    "for horizon in horizons:\n",
    "    # 1) load and copy base test set\n",
    "    print(f\"\\n=== Horizon {horizon} ===\")\n",
    "    base_df = test_data[horizon]\n",
    "    \n",
    "    # 2) determine your features once\n",
    "    exclude_cols = [\n",
    "        'iv_bs_error','iv_ahbs','iv_ahbs_error','iv_bs','iv_cw','iv_cw_error',\n",
    "        'impl_volatility','moneyness_category','ID','date','new_id',\n",
    "        'open_interest','option_price','prediction_horizon',\n",
    "        'test_date','train_date','volume'\n",
    "    ]\n",
    "    feature_columns = [c for c in base_df.columns if c not in exclude_cols]\n",
    "    \n",
    "    # 3) loop over error types _and_ models, but update the SAME DataFrame\n",
    "    df = base_df\n",
    "    for error_type in ['bs','ahbs','cw']:\n",
    "        print(f\"\\n-- predicting corrections for {error_type} --\")\n",
    "        \n",
    "        models_for_type = loaded_models[error_type]  # e.g. [nn3_model, nn4_model]\n",
    "        \n",
    "        # assume your helper loops through each model internally,\n",
    "        # adding both iv_{error_type}_pred_NN3/_NN4 and iv_{error_type}_corrected_NN3/_NN4\n",
    "        df = predict_and_add_to_test_data(\n",
    "            models_for_type, df, feature_columns, scaler, error_type\n",
    "        )\n",
    "        \n",
    "        # show a quick peek\n",
    "        cols = ['impl_volatility', f'iv_{error_type}']\n",
    "        for m in ['NN3','NN4']:\n",
    "            cols += [f'iv_{error_type}_pred_{m}', f'iv_{error_type}_corrected_{m}']\n",
    "        print(df[cols].head(3))\n",
    "    \n",
    "    # 4) after all error types, assign once\n",
    "    test_data[horizon] = df\n",
    "    print(f\"\\n-- test_data_h{horizon} updated --\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bfa4a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved combined results to ivrmse_nm_nn_f_f.csv\n"
     ]
    }
   ],
   "source": [
    "def calculate_ivrmse(predictions_df, error_types=['bs', 'ahbs', 'cw'], models=['NN3', 'NN4']):\n",
    "    results = {}\n",
    "    \n",
    "    # Calculate IVRMSE for each error type and model\n",
    "    for error_type in error_types:\n",
    "        orig_col = f'iv_{error_type}'\n",
    "        \n",
    "        # Calculate base IVRMSE (before correction)\n",
    "        base_rmse = np.sqrt(mean_squared_error(predictions_df['impl_volatility'], predictions_df[orig_col]))\n",
    "        results[f\"{error_type}_base\"] = base_rmse\n",
    "        \n",
    "        # Calculate IVRMSE for each model\n",
    "        for model in models:\n",
    "            corrected_col = f'iv_{error_type}_corrected_{model}'\n",
    "            \n",
    "            # Skip if corrected column doesn't exist\n",
    "            if corrected_col not in predictions_df.columns:\n",
    "                print(f\"Warning: {corrected_col} column not found, skipping...\")\n",
    "                continue\n",
    "                \n",
    "            # Calculate IVRMSE for the corrected predictions\n",
    "            corrected_diff = predictions_df['impl_volatility'] - predictions_df[corrected_col]\n",
    "            corrected_rmse = np.sqrt(mean_squared_error(predictions_df['impl_volatility'], predictions_df[corrected_col]))\n",
    "            results[f\"{error_type}_{model}\"] = corrected_rmse\n",
    "            \n",
    "            # Calculate improvement percentage\n",
    "            improvement = (base_rmse - corrected_rmse) / base_rmse * 100\n",
    "            results[f\"{error_type}_{model}_improvement\"] = improvement\n",
    "    \n",
    "    return results\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def build_vertical_ivrmse_table(horizons, test_data):\n",
    "    all_panels = []\n",
    "\n",
    "    for horizon in horizons:\n",
    "        ivrmse_results = calculate_ivrmse(test_data[horizon])\n",
    "        records = []\n",
    "        for error_type in ['bs', 'ahbs', 'cw']:\n",
    "            row = []\n",
    "            base_key = f\"{error_type}_base\"\n",
    "            row.append(ivrmse_results.get(base_key, None))\n",
    "            for model in ['NN3', 'NN4']:\n",
    "                model_key = f\"{error_type}_{model}\"\n",
    "                row.append(ivrmse_results.get(model_key, None))\n",
    "            records.append(row)\n",
    "\n",
    "        df = pd.DataFrame(records, \n",
    "                          index=['BS', 'AHBS', 'CW'], \n",
    "                          columns=['Base', 'NN3', 'NN4'])\n",
    "        df['Horizon'] = horizon\n",
    "        all_panels.append(df.reset_index())\n",
    "\n",
    "    combined_df = pd.concat(all_panels)\n",
    "    combined_df.rename(columns={'index': 'Error_Type'}, inplace=True)\n",
    "\n",
    "    # Save to CSV\n",
    "    output_path = \"ivrmse_nm_nn_f_f.csv\"\n",
    "    combined_df.to_csv(output_path, index=False)\n",
    "    print(f\"Saved combined results to {output_path}\")\n",
    "\n",
    "    return combined_df\n",
    "\n",
    "# Generate and save\n",
    "combined_csv_df = build_vertical_ivrmse_table(horizons, test_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f2b159",
   "metadata": {},
   "source": [
    "Calculate IVRMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62d44d12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added moneyness categories to test_data_per_horizon[1]\n",
      "\n",
      "Sample data for horizon 1:\n",
      "   cp_flag  moneyness moneyness_category\n",
      "0        1   0.996078                ATM\n",
      "1        1   0.987564                ATM\n",
      "2        1   0.979195                ATM\n",
      "3        1   0.970966                ATM\n",
      "4        1   0.962875               OTMC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxva\\AppData\\Local\\Temp\\ipykernel_29380\\3688721833.py:16: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'DOTMC' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  result.loc[dotmc_mask, 'moneyness_category'] = 'DOTMC'\n",
      "C:\\Users\\maxva\\AppData\\Local\\Temp\\ipykernel_29380\\3688721833.py:16: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'DOTMC' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  result.loc[dotmc_mask, 'moneyness_category'] = 'DOTMC'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added moneyness categories to test_data_per_horizon[5]\n",
      "\n",
      "Sample data for horizon 5:\n",
      "   cp_flag  moneyness moneyness_category\n",
      "0        1   0.996078                ATM\n",
      "1        1   0.987564                ATM\n",
      "2        1   0.979195                ATM\n",
      "3        1   0.970966                ATM\n",
      "4        1   0.962875               OTMC\n",
      "Added moneyness categories to test_data_per_horizon[21]\n",
      "\n",
      "Sample data for horizon 21:\n",
      "   cp_flag  moneyness moneyness_category\n",
      "0        1   0.962875               OTMC\n",
      "1        1   0.924360               OTMC\n",
      "2        1   0.888808              DOTMC\n",
      "3        1   0.855889              DOTMC\n",
      "4        0   1.155450              DOTMP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxva\\AppData\\Local\\Temp\\ipykernel_29380\\3688721833.py:16: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'DOTMC' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  result.loc[dotmc_mask, 'moneyness_category'] = 'DOTMC'\n"
     ]
    }
   ],
   "source": [
    "def add_moneyness_categories(df):\n",
    "    # Create a copy to avoid modifying the original dataframe\n",
    "    result = df\n",
    "    \n",
    "    # Initialize moneyness_category column with NaN values\n",
    "    result['moneyness_category'] = np.nan\n",
    "    \n",
    "    # Define conditions for each category\n",
    "    dotmc_mask = (result['cp_flag'] == 1) & (result['moneyness'] >= 0.80) & (result['moneyness'] < 0.90)\n",
    "    otmc_mask = (result['cp_flag'] == 1) & (result['moneyness'] >= 0.90) & (result['moneyness'] < 0.97)\n",
    "    atm_mask = (result['moneyness'] >= 0.97) & (result['moneyness'] < 1.03)\n",
    "    otmp_mask = (result['cp_flag'] == 0) & (result['moneyness'] >= 1.03) & (result['moneyness'] < 1.10)\n",
    "    dotmp_mask = (result['cp_flag'] == 0) & (result['moneyness'] >= 1.10) & (result['moneyness'] <= 1.60)\n",
    "    \n",
    "    # Assign categories based on conditions\n",
    "    result.loc[dotmc_mask, 'moneyness_category'] = 'DOTMC'\n",
    "    result.loc[otmc_mask, 'moneyness_category'] = 'OTMC'\n",
    "    result.loc[atm_mask, 'moneyness_category'] = 'ATM'\n",
    "    result.loc[otmp_mask, 'moneyness_category'] = 'OTMP'\n",
    "    result.loc[dotmp_mask, 'moneyness_category'] = 'DOTMP'\n",
    "    \n",
    "    return result\n",
    "\n",
    "for horizon in [1, 5, 21]:\n",
    "    # Apply moneyness categorization\n",
    "    test_data[horizon] = add_moneyness_categories(test_data[horizon])\n",
    "    print(f\"Added moneyness categories to test_data_per_horizon[{horizon}]\")\n",
    "        \n",
    "     # Print sample data to verify\n",
    "    display_columns = ['cp_flag', 'moneyness', 'moneyness_category']\n",
    "    print(f\"\\nSample data for horizon {horizon}:\")\n",
    "    print(test_data[horizon][display_columns].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6897393",
   "metadata": {},
   "source": [
    "ivrmse per moneyness group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05488a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'DOTMC': {'bs_base': np.float64(0.12766540322357306), 'bs_NN3': np.float64(0.02990797398289615), 'bs_NN3_improvement': np.float64(76.57315668324014), 'bs_NN4': np.float64(0.030109544621635294), 'bs_NN4_improvement': np.float64(76.41526689191889), 'ahbs_base': np.float64(0.12243075572812812), 'ahbs_NN3': np.float64(0.029705627969835405), 'ahbs_NN3_improvement': np.float64(75.73679277468463), 'ahbs_NN4': np.float64(0.02995692981343165), 'ahbs_NN4_improvement': np.float64(75.53153238720952), 'cw_base': np.float64(0.11351207227281072), 'cw_NN3': np.float64(0.029665086728749367), 'cw_NN3_improvement': np.float64(73.86613940281754), 'cw_NN4': np.float64(0.02973472216861714), 'cw_NN4_improvement': np.float64(73.80479311737538)}, 'OTMC': {'bs_base': np.float64(0.11783968278540764), 'bs_NN3': np.float64(0.02468646867595163), 'bs_NN3_improvement': np.float64(79.05080182462217), 'bs_NN4': np.float64(0.02487454276533491), 'bs_NN4_improvement': np.float64(78.89120016503034), 'ahbs_base': np.float64(0.11010225595643366), 'ahbs_NN3': np.float64(0.024557674941444105), 'ahbs_NN3_improvement': np.float64(77.6955751468332), 'ahbs_NN4': np.float64(0.024718543704807458), 'ahbs_NN4_improvement': np.float64(77.54946663891398), 'cw_base': np.float64(0.10871369168569855), 'cw_NN3': np.float64(0.024201994109991336), 'cw_NN3_improvement': np.float64(77.73786012164726), 'cw_NN4': np.float64(0.024199407607384545), 'cw_NN4_improvement': np.float64(77.7402393091872)}, 'ATM': {'bs_base': np.float64(0.11533451774299444), 'bs_NN3': np.float64(0.024094502037761655), 'bs_NN3_improvement': np.float64(79.10902780080754), 'bs_NN4': np.float64(0.02438162445514964), 'bs_NN4_improvement': np.float64(78.86008028448134), 'ahbs_base': np.float64(0.11099006338240232), 'ahbs_NN3': np.float64(0.024319132209145805), 'ahbs_NN3_improvement': np.float64(78.08891042312743), 'ahbs_NN4': np.float64(0.02435410263167606), 'ahbs_NN4_improvement': np.float64(78.0574027174243), 'cw_base': np.float64(0.10887977980930774), 'cw_NN3': np.float64(0.02413373083361842), 'cw_NN3_improvement': np.float64(77.83451539313701), 'cw_NN4': np.float64(0.02415137379797474), 'cw_NN4_improvement': np.float64(77.81831131521986)}, 'OTMP': {'bs_base': np.float64(0.10847991577680464), 'bs_NN3': np.float64(0.024692185505392927), 'bs_NN3_improvement': np.float64(77.23801191347104), 'bs_NN4': np.float64(0.024723007572974614), 'bs_NN4_improvement': np.float64(77.20959921849338), 'ahbs_base': np.float64(0.10624695173131551), 'ahbs_NN3': np.float64(0.02504433283643852), 'ahbs_NN3_improvement': np.float64(76.42818694716784), 'ahbs_NN4': np.float64(0.025036016149431968), 'ahbs_NN4_improvement': np.float64(76.43601464186499), 'cw_base': np.float64(0.10522766281872971), 'cw_NN3': np.float64(0.024727799610268025), 'cw_NN3_improvement': np.float64(76.50066631921176), 'cw_NN4': np.float64(0.024750951778061994), 'cw_NN4_improvement': np.float64(76.47866434066944)}, 'DOTMP': {'bs_base': np.float64(0.169022903936014), 'bs_NN3': np.float64(0.03165187338595821), 'bs_NN3_improvement': np.float64(81.27361875291146), 'bs_NN4': np.float64(0.031762351956481215), 'bs_NN4_improvement': np.float64(81.20825567610333), 'ahbs_base': np.float64(0.11760650193426411), 'ahbs_NN3': np.float64(0.03435058326430396), 'ahbs_NN3_improvement': np.float64(70.79193522522748), 'ahbs_NN4': np.float64(0.0344878888007113), 'ahbs_NN4_improvement': np.float64(70.67518527165426), 'cw_base': np.float64(0.11209674457356614), 'cw_NN3': np.float64(0.03419379161724193), 'cw_NN3_improvement': np.float64(69.49617783520785), 'cw_NN4': np.float64(0.0342691996077924), 'cw_NN4_improvement': np.float64(69.4289073798192)}}\n"
     ]
    }
   ],
   "source": [
    "moneyness_groups = ['DOTMC', 'OTMC', 'ATM', 'OTMP', 'DOTMP']\n",
    "\n",
    "def analyze_ivrmse_by_moneyness(test_data):\n",
    "\n",
    "    # 1. Group the test data by moneyness\n",
    "    results = {}\n",
    "    for group in moneyness_groups:\n",
    "        group_data = test_data[test_data['moneyness_category'] == group]\n",
    "        \n",
    "        # Skip if no data in this group\n",
    "        if len(group_data) == 0:\n",
    "            print(f\"Warning: No data found for moneyness group {group}\")\n",
    "            continue\n",
    "            \n",
    "        # Initialize results for this group\n",
    "        results[group] = {}\n",
    "        \n",
    "        # 2. Calculate IVRMSE for each error type and model\n",
    "        for error_type in ['bs', 'ahbs', 'cw']:\n",
    "            # Column names\n",
    "            orig_col = f'iv_{error_type}'\n",
    "            \n",
    "            # Skip if column doesn't exist\n",
    "            if orig_col not in group_data.columns:\n",
    "                print(f\"Warning: Column {orig_col} not found, skipping...\")\n",
    "                continue\n",
    "                \n",
    "            # Calculate baseline IVRMSE\n",
    "            base_rmse = np.sqrt(mean_squared_error(\n",
    "                group_data['impl_volatility'], \n",
    "                group_data[orig_col]\n",
    "            ))\n",
    "            \n",
    "            # Store baseline IVRMSE\n",
    "            results[group][f\"{error_type}_base\"] = base_rmse\n",
    "            \n",
    "            # Calculate corrected IVRMSE for each model\n",
    "            for model in ['NN3', 'NN4']:\n",
    "                corrected_col = f'iv_{error_type}_corrected_{model}'\n",
    "                \n",
    "                # Skip if column doesn't exist\n",
    "                if corrected_col not in group_data.columns:\n",
    "                    print(f\"Warning: Column {corrected_col} not found, skipping...\")\n",
    "                    continue\n",
    "                    \n",
    "                corrected_rmse = np.sqrt(mean_squared_error(\n",
    "                    group_data['impl_volatility'], \n",
    "                    group_data[corrected_col]\n",
    "                ))\n",
    "                \n",
    "                # Calculate improvement percentage\n",
    "                improvement = (base_rmse - corrected_rmse) / base_rmse * 100\n",
    "                \n",
    "                # Store results\n",
    "                results[group][f\"{error_type}_{model}\"] = corrected_rmse\n",
    "                results[group][f\"{error_type}_{model}_improvement\"] = improvement\n",
    "    \n",
    "    return results\n",
    "\n",
    "def format_results_table(results):\n",
    "    # Prepare IVRMSE table data\n",
    "    ivrmse_data = []\n",
    "    for group, group_results in results.items():\n",
    "        row = {'Moneyness Group': group}\n",
    "        \n",
    "        for key, value in group_results.items():\n",
    "            if not key.endswith('improvement'):\n",
    "                if key.endswith('base'):\n",
    "                    # Format baseline columns\n",
    "                    error_type = key.split('_')[0].upper()\n",
    "                    row[f\"{error_type} Base\"] = value\n",
    "                else:\n",
    "                    # Format model columns\n",
    "                    parts = key.split('_')\n",
    "                    error_type = parts[0].upper()\n",
    "                    model = parts[1]\n",
    "                    row[f\"{error_type} {model}\"] = value\n",
    "        \n",
    "        ivrmse_data.append(row)\n",
    "    \n",
    "    # Prepare improvement percentage table data\n",
    "    improvement_data = []\n",
    "    for group, group_results in results.items():\n",
    "        row = {'Moneyness Group': group}\n",
    "        \n",
    "        for key, value in group_results.items():\n",
    "            if key.endswith('improvement'):\n",
    "                # Format improvement columns\n",
    "                parts = key.replace('_improvement', '').split('_')\n",
    "                error_type = parts[0].upper()\n",
    "                model = parts[1]\n",
    "                row[f\"{error_type} {model}\"] = value\n",
    "        \n",
    "        improvement_data.append(row)\n",
    "    \n",
    "    # Create DataFrames\n",
    "    ivrmse_df = pd.DataFrame(ivrmse_data)\n",
    "    improvement_df = pd.DataFrame(improvement_data)\n",
    "    \n",
    "    # Sort columns for better readability\n",
    "    ivrmse_cols = ['Moneyness Group']\n",
    "    improvement_cols = ['Moneyness Group']\n",
    "    \n",
    "    for et in ['BS', 'AHBS', 'CW']:\n",
    "        ivrmse_cols.extend([f\"{et} Base\", f\"{et} NN3\", f\"{et} NN4\"])\n",
    "        improvement_cols.extend([f\"{et} NN3\", f\"{et} NN4\"])\n",
    "    \n",
    "    # Reorder columns if they exist\n",
    "    ivrmse_df = ivrmse_df[[col for col in ivrmse_cols if col in ivrmse_df.columns]]\n",
    "    improvement_df = improvement_df[[col for col in improvement_cols if col in improvement_df.columns]]\n",
    "    \n",
    "    return ivrmse_df, improvement_df\n",
    "\n",
    "def find_best_models(results):\n",
    "    best_models = []\n",
    "    \n",
    "    for group, group_results in results.items():\n",
    "        for error_type in ['bs', 'ahbs', 'cw']:\n",
    "            # Get baseline IVRMSE\n",
    "            base_key = f\"{error_type}_base\"\n",
    "            if base_key not in group_results:\n",
    "                continue\n",
    "                \n",
    "            base_rmse = group_results[base_key]\n",
    "            \n",
    "            # Find best model for this error type\n",
    "            models = [m for m in ['NN3', 'NN4'] if f\"{error_type}_{m}\" in group_results]\n",
    "            if not models:\n",
    "                continue\n",
    "            \n",
    "            # Find model with lowest IVRMSE    \n",
    "            best_model = min(models, key=lambda m: group_results[f\"{error_type}_{m}\"])\n",
    "            best_rmse = group_results[f\"{error_type}_{best_model}\"]\n",
    "            improvement = group_results[f\"{error_type}_{best_model}_improvement\"]\n",
    "            \n",
    "            # Also get the values for the other model for comparison\n",
    "            other_models = [m for m in models if m != best_model]\n",
    "            other_model_data = {}\n",
    "            if other_models:\n",
    "                other_model = other_models[0]\n",
    "                other_rmse = group_results[f\"{error_type}_{other_model}\"]\n",
    "                other_improvement = group_results[f\"{error_type}_{other_model}_improvement\"]\n",
    "                other_model_data = {\n",
    "                    f'Other Model': other_model,\n",
    "                    f'Other IVRMSE': other_rmse,\n",
    "                    f'Other Improvement %': other_improvement\n",
    "                }\n",
    "            \n",
    "            model_data = {\n",
    "                'Moneyness Group': group,\n",
    "                'Error Type': error_type.upper(),\n",
    "                'Best Model': best_model,\n",
    "                'Base IVRMSE': base_rmse,\n",
    "                'Best IVRMSE': best_rmse,\n",
    "                'Improvement %': improvement\n",
    "            }\n",
    "            \n",
    "            # Add other model data if available\n",
    "            model_data.update(other_model_data)\n",
    "            \n",
    "            best_models.append(model_data)\n",
    "    \n",
    "    return pd.DataFrame(best_models)\n",
    "\n",
    "def print_formatted_tables(results):\n",
    "\n",
    "    # Format results into DataFrames\n",
    "    ivrmse_df, improvement_df = format_results_table(results)\n",
    "    \n",
    "    # Format IVRMSE table\n",
    "    pd.set_option('display.float_format', '{:.6f}'.format)\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"IVRMSE by Moneyness Group (Horizon: {horizon})\")\n",
    "    print(\"=\" * 80)\n",
    "    print(ivrmse_df.to_string(index=False))\n",
    "    \n",
    "    # Format improvement table\n",
    "    pd.set_option('display.float_format', '{:.2f}%'.format)\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"Improvement Percentage by Moneyness Group\")\n",
    "    print(\"=\" * 80)\n",
    "    print(improvement_df.to_string(index=False))\n",
    "    \n",
    "    # Find best models\n",
    "    best_models_df = find_best_models(results)\n",
    "    \n",
    "    # Reset float format for mixed table\n",
    "    pd.set_option('display.float_format', None)\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"Best Model by Moneyness Group and Error Type\")\n",
    "    print(\"=\" * 80)\n",
    "    # Format specific columns\n",
    "    best_models_df['Base IVRMSE'] = best_models_df['Base IVRMSE'].map('{:.6f}'.format)\n",
    "    best_models_df['Best IVRMSE'] = best_models_df['Best IVRMSE'].map('{:.6f}'.format)\n",
    "    best_models_df['Improvement %'] = best_models_df['Improvement %'].map('{:.2f}%'.format)\n",
    "    print(best_models_df.to_string(index=False))\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"Summary Statistics\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Count best model occurrences\n",
    "    model_counts = best_models_df['Best Model'].value_counts()\n",
    "    print(f\"Overall best model distribution: {dict(model_counts)}\")\n",
    "    \n",
    "    # Average improvement by moneyness group\n",
    "    print(\"\\nAverage improvement by moneyness group:\")\n",
    "    # Convert percentage strings to numeric values\n",
    "    best_models_df['Improvement_Numeric'] = pd.to_numeric(best_models_df['Improvement %'].str.rstrip('%'))\n",
    "    \n",
    "    # Group and calculate means\n",
    "    avg_improvement = best_models_df.groupby('Moneyness Group')['Improvement_Numeric'].mean()\n",
    "    \n",
    "    # Handle the case where there's only one group (which returns a scalar)\n",
    "    if isinstance(avg_improvement, pd.Series):\n",
    "        # Sort if it's a Series with multiple values\n",
    "        sorted_improvements = avg_improvement.sort_values(ascending=False)\n",
    "        for group, imp in sorted_improvements.items():\n",
    "            print(f\"  {group}: {imp:.2f}%\")\n",
    "    else:\n",
    "        # Just print the single value if it's a scalar\n",
    "        group = best_models_df['Moneyness Group'].iloc[0]\n",
    "        print(f\"  {group}: {avg_improvement:.2f}%\")\n",
    "\n",
    "# Example usage:\n",
    "results_h1 = analyze_ivrmse_by_moneyness(test_data[1])\n",
    "results_h5 = analyze_ivrmse_by_moneyness(test_data[5])\n",
    "results_h21 = analyze_ivrmse_by_moneyness(test_data[21])\n",
    "print(results_h1)\n",
    "\n",
    "# To get the results as DataFrames for further analysis:\n",
    "#ivrmse_df, improvement_df = format_results_table(results)\n",
    "#best_models_df = find_best_models(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bff22d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "IVRMSE by Moneyness Group (Horizon: 1)\n",
      "================================================================================\n",
      "Moneyness Group  BS Base   BS NN3   BS NN4  AHBS Base  AHBS NN3  AHBS NN4  CW Base   CW NN3   CW NN4\n",
      "          DOTMC 0.127665 0.029908 0.030110   0.122431  0.029706  0.029957 0.113512 0.029665 0.029735\n",
      "           OTMC 0.117840 0.024686 0.024875   0.110102  0.024558  0.024719 0.108714 0.024202 0.024199\n",
      "            ATM 0.115335 0.024095 0.024382   0.110990  0.024319  0.024354 0.108880 0.024134 0.024151\n",
      "           OTMP 0.108480 0.024692 0.024723   0.106247  0.025044  0.025036 0.105228 0.024728 0.024751\n",
      "          DOTMP 0.169023 0.031652 0.031762   0.117607  0.034351  0.034488 0.112097 0.034194 0.034269\n",
      "\n",
      "================================================================================\n",
      "Improvement Percentage by Moneyness Group\n",
      "================================================================================\n",
      "Moneyness Group  BS NN3  BS NN4  AHBS NN3  AHBS NN4  CW NN3  CW NN4\n",
      "          DOTMC  76.57%  76.42%    75.74%    75.53%  73.87%  73.80%\n",
      "           OTMC  79.05%  78.89%    77.70%    77.55%  77.74%  77.74%\n",
      "            ATM  79.11%  78.86%    78.09%    78.06%  77.83%  77.82%\n",
      "           OTMP  77.24%  77.21%    76.43%    76.44%  76.50%  76.48%\n",
      "          DOTMP  81.27%  81.21%    70.79%    70.68%  69.50%  69.43%\n",
      "\n",
      "================================================================================\n",
      "Best Model by Moneyness Group and Error Type\n",
      "================================================================================\n",
      "Moneyness Group Error Type Best Model Base IVRMSE Best IVRMSE Improvement % Other Model  Other IVRMSE  Other Improvement %\n",
      "          DOTMC         BS        NN3    0.127665    0.029908        76.57%         NN4      0.030110            76.415267\n",
      "          DOTMC       AHBS        NN3    0.122431    0.029706        75.74%         NN4      0.029957            75.531532\n",
      "          DOTMC         CW        NN3    0.113512    0.029665        73.87%         NN4      0.029735            73.804793\n",
      "           OTMC         BS        NN3    0.117840    0.024686        79.05%         NN4      0.024875            78.891200\n",
      "           OTMC       AHBS        NN3    0.110102    0.024558        77.70%         NN4      0.024719            77.549467\n",
      "           OTMC         CW        NN4    0.108714    0.024199        77.74%         NN3      0.024202            77.737860\n",
      "            ATM         BS        NN3    0.115335    0.024095        79.11%         NN4      0.024382            78.860080\n",
      "            ATM       AHBS        NN3    0.110990    0.024319        78.09%         NN4      0.024354            78.057403\n",
      "            ATM         CW        NN3    0.108880    0.024134        77.83%         NN4      0.024151            77.818311\n",
      "           OTMP         BS        NN3    0.108480    0.024692        77.24%         NN4      0.024723            77.209599\n",
      "           OTMP       AHBS        NN4    0.106247    0.025036        76.44%         NN3      0.025044            76.428187\n",
      "           OTMP         CW        NN3    0.105228    0.024728        76.50%         NN4      0.024751            76.478664\n",
      "          DOTMP         BS        NN3    0.169023    0.031652        81.27%         NN4      0.031762            81.208256\n",
      "          DOTMP       AHBS        NN3    0.117607    0.034351        70.79%         NN4      0.034488            70.675185\n",
      "          DOTMP         CW        NN3    0.112097    0.034194        69.50%         NN4      0.034269            69.428907\n",
      "\n",
      "================================================================================\n",
      "Summary Statistics\n",
      "================================================================================\n",
      "================================================================================\n",
      "IVRMSE by Moneyness Group (Horizon: 5)\n",
      "================================================================================\n",
      "Moneyness Group  BS Base   BS NN3   BS NN4  AHBS Base  AHBS NN3  AHBS NN4  CW Base   CW NN3   CW NN4\n",
      "          DOTMC 0.128016 0.045485 0.045756   0.121934  0.044853  0.045002 0.114773 0.044832 0.044843\n",
      "           OTMC 0.118611 0.038416 0.038799   0.110421  0.037964  0.038035 0.109203 0.037690 0.037686\n",
      "            ATM 0.115930 0.038140 0.038553   0.111431  0.037743  0.037726 0.109988 0.037803 0.037761\n",
      "           OTMP 0.109394 0.039419 0.039670   0.107404  0.038914  0.039010 0.106494 0.038956 0.038951\n",
      "          DOTMP 0.167076 0.049546 0.049736   0.119559  0.051165  0.051338 0.114737 0.050988 0.051142\n",
      "\n",
      "================================================================================\n",
      "Improvement Percentage by Moneyness Group\n",
      "================================================================================\n",
      "Moneyness Group  BS NN3  BS NN4  AHBS NN3  AHBS NN4  CW NN3  CW NN4\n",
      "          DOTMC  64.47%  64.26%    63.22%    63.09%  60.94%  60.93%\n",
      "           OTMC  67.61%  67.29%    65.62%    65.55%  65.49%  65.49%\n",
      "            ATM  67.10%  66.74%    66.13%    66.14%  65.63%  65.67%\n",
      "           OTMP  63.97%  63.74%    63.77%    63.68%  63.42%  63.42%\n",
      "          DOTMP  70.35%  70.23%    57.21%    57.06%  55.56%  55.43%\n",
      "\n",
      "================================================================================\n",
      "Best Model by Moneyness Group and Error Type\n",
      "================================================================================\n",
      "Moneyness Group Error Type Best Model Base IVRMSE Best IVRMSE Improvement % Other Model  Other IVRMSE  Other Improvement %\n",
      "          DOTMC         BS        NN3    0.128016    0.045485        64.47%         NN4      0.045756            64.257589\n",
      "          DOTMC       AHBS        NN3    0.121934    0.044853        63.22%         NN4      0.045002            63.092882\n",
      "          DOTMC         CW        NN3    0.114773    0.044832        60.94%         NN4      0.044843            60.929208\n",
      "           OTMC         BS        NN3    0.118611    0.038416        67.61%         NN4      0.038799            67.288949\n",
      "           OTMC       AHBS        NN3    0.110421    0.037964        65.62%         NN4      0.038035            65.554983\n",
      "           OTMC         CW        NN4    0.109203    0.037686        65.49%         NN3      0.037690            65.486314\n",
      "            ATM         BS        NN3    0.115930    0.038140        67.10%         NN4      0.038553            66.744341\n",
      "            ATM       AHBS        NN4    0.111431    0.037726        66.14%         NN3      0.037743            66.129074\n",
      "            ATM         CW        NN4    0.109988    0.037761        65.67%         NN3      0.037803            65.629572\n",
      "           OTMP         BS        NN3    0.109394    0.039419        63.97%         NN4      0.039670            63.737111\n",
      "           OTMP       AHBS        NN3    0.107404    0.038914        63.77%         NN4      0.039010            63.678796\n",
      "           OTMP         CW        NN4    0.106494    0.038951        63.42%         NN3      0.038956            63.419916\n",
      "          DOTMP         BS        NN3    0.167076    0.049546        70.35%         NN4      0.049736            70.231478\n",
      "          DOTMP       AHBS        NN3    0.119559    0.051165        57.21%         NN4      0.051338            57.060853\n",
      "          DOTMP         CW        NN3    0.114737    0.050988        55.56%         NN4      0.051142            55.426486\n",
      "\n",
      "================================================================================\n",
      "Summary Statistics\n",
      "================================================================================\n",
      "================================================================================\n",
      "IVRMSE by Moneyness Group (Horizon: 21)\n",
      "================================================================================\n",
      "Moneyness Group  BS Base   BS NN3   BS NN4  AHBS Base  AHBS NN3  AHBS NN4  CW Base   CW NN3   CW NN4\n",
      "          DOTMC 0.131733 0.070253 0.070071   0.122918  0.068916  0.068999 0.118992 0.067212 0.067389\n",
      "           OTMC 0.121774 0.062842 0.063481   0.112418  0.061958  0.061675 0.111954 0.061545 0.061487\n",
      "            ATM 0.118979 0.062944 0.063389   0.114234  0.061968  0.061543 0.114215 0.062499 0.062425\n",
      "           OTMP 0.114140 0.063774 0.064165   0.112578  0.062853  0.062610 0.111846 0.062856 0.063062\n",
      "          DOTMP 0.164484 0.077169 0.077852   0.124987  0.076534  0.076667 0.120170 0.074640 0.074957\n",
      "\n",
      "================================================================================\n",
      "Improvement Percentage by Moneyness Group\n",
      "================================================================================\n",
      "Moneyness Group  BS NN3  BS NN4  AHBS NN3  AHBS NN4  CW NN3  CW NN4\n",
      "          DOTMC  46.67%  46.81%    43.93%    43.87%  43.52%  43.37%\n",
      "           OTMC  48.39%  47.87%    44.89%    45.14%  45.03%  45.08%\n",
      "            ATM  47.10%  46.72%    45.75%    46.13%  45.28%  45.34%\n",
      "           OTMP  44.13%  43.78%    44.17%    44.39%  43.80%  43.62%\n",
      "          DOTMP  53.08%  52.67%    38.77%    38.66%  37.89%  37.62%\n",
      "\n",
      "================================================================================\n",
      "Best Model by Moneyness Group and Error Type\n",
      "================================================================================\n",
      "Moneyness Group Error Type Best Model Base IVRMSE Best IVRMSE Improvement % Other Model  Other IVRMSE  Other Improvement %\n",
      "          DOTMC         BS        NN4    0.131733    0.070071        46.81%         NN3      0.070253            46.670481\n",
      "          DOTMC       AHBS        NN3    0.122918    0.068916        43.93%         NN4      0.068999            43.865946\n",
      "          DOTMC         CW        NN3    0.118992    0.067212        43.52%         NN4      0.067389            43.366178\n",
      "           OTMC         BS        NN3    0.121774    0.062842        48.39%         NN4      0.063481            47.870105\n",
      "           OTMC       AHBS        NN4    0.112418    0.061675        45.14%         NN3      0.061958            44.886464\n",
      "           OTMC         CW        NN4    0.111954    0.061487        45.08%         NN3      0.061545            45.027028\n",
      "            ATM         BS        NN3    0.118979    0.062944        47.10%         NN4      0.063389            46.722304\n",
      "            ATM       AHBS        NN4    0.114234    0.061543        46.13%         NN3      0.061968            45.753356\n",
      "            ATM         CW        NN4    0.114215    0.062425        45.34%         NN3      0.062499            45.279989\n",
      "           OTMP         BS        NN3    0.114140    0.063774        44.13%         NN4      0.064165            43.783510\n",
      "           OTMP       AHBS        NN4    0.112578    0.062610        44.39%         NN3      0.062853            44.169425\n",
      "           OTMP         CW        NN3    0.111846    0.062856        43.80%         NN4      0.063062            43.617099\n",
      "          DOTMP         BS        NN3    0.164484    0.077169        53.08%         NN4      0.077852            52.669131\n",
      "          DOTMP       AHBS        NN3    0.124987    0.076534        38.77%         NN4      0.076667            38.660302\n",
      "          DOTMP         CW        NN3    0.120170    0.074640        37.89%         NN4      0.074957            37.624001\n",
      "\n",
      "================================================================================\n",
      "Summary Statistics\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# 1) Change the signature to accept horizon\n",
    "def print_formatted_tables(results, horizon):\n",
    "\n",
    "    ivrmse_df, improvement_df = format_results_table(results)\n",
    "    \n",
    "    # IVRMSE table\n",
    "    pd.set_option('display.float_format', '{:.6f}'.format)\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"IVRMSE by Moneyness Group (Horizon: {horizon})\")\n",
    "    print(\"=\" * 80)\n",
    "    print(ivrmse_df.to_string(index=False))\n",
    "\n",
    "    # Improvement %\n",
    "    pd.set_option('display.float_format', '{:.2f}%'.format)\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"Improvement Percentage by Moneyness Group\")\n",
    "    print(\"=\" * 80)\n",
    "    print(improvement_df.to_string(index=False))\n",
    "\n",
    "    # Best models & summary (same as before)…\n",
    "    best_models_df = find_best_models(results)\n",
    "    pd.set_option('display.float_format', None)\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"Best Model by Moneyness Group and Error Type\")\n",
    "    print(\"=\" * 80)\n",
    "    best_models_df['Base IVRMSE']    = best_models_df['Base IVRMSE'].map('{:.6f}'.format)\n",
    "    best_models_df['Best IVRMSE']    = best_models_df['Best IVRMSE'].map('{:.6f}'.format)\n",
    "    best_models_df['Improvement %']  = best_models_df['Improvement %'].map('{:.2f}%'.format)\n",
    "    print(best_models_df.to_string(index=False))\n",
    "\n",
    "    # Summary statistics (same as before)…\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"Summary Statistics\")\n",
    "    print(\"=\" * 80)\n",
    "    # (rest of your summary code)\n",
    "\n",
    "# 2) Now loop over all horizons:\n",
    "all_results = {\n",
    "    1: analyze_ivrmse_by_moneyness(test_data[1]),\n",
    "    5: analyze_ivrmse_by_moneyness(test_data[5]),\n",
    "    21: analyze_ivrmse_by_moneyness(test_data[21])\n",
    "}\n",
    "\n",
    "for h, res in all_results.items():\n",
    "    print_formatted_tables(res, h)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6d061478",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# get the table\n",
    "dfm, _ = format_results_table(all_results[1])\n",
    "dfm5, _ = format_results_table(all_results[5])\n",
    "dfm21, _ = format_results_table(all_results[21])\n",
    "\n",
    "# write to CSV (no index column)\n",
    "dfm.to_csv('results_1_nn_f_f.csv', index=False)\n",
    "dfm5.to_csv('results_5_nn_f_f.csv', index=False)\n",
    "dfm21.to_csv('results_21_nn_f_f.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cb16b4",
   "metadata": {},
   "source": [
    "Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efeb20e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 most important features for NN3 - BS Error Correction:\n",
      "            Feature  Importance\n",
      "8             theta    2.906260\n",
      "4      strike_price    1.682651\n",
      "6             gamma    1.570778\n",
      "1       stock_price    1.534537\n",
      "7              vega    1.441805\n",
      "3    time_to_expiry    1.385853\n",
      "2         moneyness    1.133738\n",
      "9                rf    1.052882\n",
      "0           cp_flag    0.874547\n",
      "91           pi_nix    0.870244\n",
      "150      corr_1260d    0.808839\n",
      "141   turnover_126d    0.806352\n",
      "151   betabab_1260d    0.765111\n",
      "103        kz_index    0.763384\n",
      "83       dsale_dsga    0.752337\n",
      "105         ni_ivol    0.734855\n",
      "145        ami_126d    0.731005\n",
      "153             age    0.730412\n",
      "44        ncol_gr1a    0.718287\n",
      "60     oaccruals_ni    0.712396\n",
      "\n",
      "=== Layer Information ===\n",
      "\n",
      "Layer 0: dense (Dense)\n",
      "  Shape: [(159, 128), (128,)]\n",
      "  Weight stats: Mean=0.0629, Std=3.8003\n",
      "  Bias stats: Mean=-1.6224, Std=7.3908\n",
      "\n",
      "Layer 1: dense_1 (Dense)\n",
      "  Shape: [(128, 64), (64,)]\n",
      "  Weight stats: Mean=-0.2404, Std=2.0106\n",
      "  Bias stats: Mean=-0.8431, Std=1.3985\n",
      "\n",
      "Layer 2: dense_2 (Dense)\n",
      "  Shape: [(64, 18), (18,)]\n",
      "  Weight stats: Mean=-0.8068, Std=5.1468\n",
      "  Bias stats: Mean=-0.9685, Std=0.8829\n",
      "\n",
      "Layer 3: dense_3 (Dense)\n",
      "  Shape: [(18, 1), (1,)]\n",
      "  Weight stats: Mean=1.5637, Std=1.2941\n",
      "  Bias stats: Mean=0.3102, Std=0.0000\n"
     ]
    }
   ],
   "source": [
    "all_models = {\n",
    "    'bs': {},\n",
    "    'ahbs': {},\n",
    "    'cw': {}\n",
    "}\n",
    "\n",
    "def analyze_neural_network_weights(model, feature_names, model_name=\"Neural Network\"):\n",
    "\n",
    "    # Get weights from the first layer\n",
    "    first_layer_weights = model.layers[0].get_weights()[0]  # Shape: (input_features, neurons)\n",
    "    \n",
    "    # Calculate importance as the mean absolute weight for each feature\n",
    "    importance = np.mean(np.abs(first_layer_weights), axis=1)\n",
    "    \n",
    "    # Normalize to sum to 100 for easier interpretation\n",
    "    importance = 100.0 * (importance / np.sum(importance))\n",
    "    \n",
    "    # Create DataFrame with feature names and importance\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': importance\n",
    "    })\n",
    "    \n",
    "    # Sort by importance\n",
    "    importance_df = importance_df.sort_values('Importance', ascending=False)\n",
    "    \n",
    "    # Print the top features\n",
    "    print(f\"Top 20 most important features for {model_name}:\")\n",
    "    print(importance_df.head(20))\n",
    "    \n",
    "    return importance_df\n",
    "\n",
    "def plot_feature_importance(importance_df, model_name=\"Neural Network\", top_n=30):\n",
    "    \"\"\"\n",
    "    Plot feature importance for the top N features.\n",
    "    \n",
    "    Parameters:\n",
    "    importance_df: DataFrame with feature importance\n",
    "    model_name: Name of the model for the plot title\n",
    "    top_n: Number of top features to plot\n",
    "    \"\"\"\n",
    "    # Select top N features\n",
    "    plot_data = importance_df.head(top_n)\n",
    "    \n",
    "    # Create horizontal bar chart\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    plt.barh(np.arange(len(plot_data)), plot_data['Importance'], align='center')\n",
    "    plt.yticks(np.arange(len(plot_data)), plot_data['Feature'])\n",
    "    plt.xlabel('Relative Importance (%)')\n",
    "    plt.title(f'Top {top_n} Features - {model_name}')\n",
    "    plt.tight_layout()\n",
    "    plt.gca().invert_yaxis()  # Invert y-axis to have the most important feature at the top\n",
    "    \n",
    "    return plt.gcf()  # Return the figure for saving if needed\n",
    "\n",
    "def calculate_permutation_importance(model, X, y, feature_names, n_repeats=10, random_state=42):\n",
    "    \"\"\"\n",
    "    Calculate permutation importance for neural network features.\n",
    "    Fixed function to avoid naming conflicts.\n",
    "    \n",
    "    Parameters:\n",
    "    model: Trained neural network model\n",
    "    X: Feature data (numpy array)\n",
    "    y: Target data\n",
    "    feature_names: List of feature names\n",
    "    n_repeats: Number of times to repeat the permutation process\n",
    "    random_state: Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame with permutation importance scores\n",
    "    \"\"\"\n",
    "    # Function to predict using the neural network\n",
    "    def predict_fn(X_data):\n",
    "        return model.predict(X_data, verbose=0).flatten()\n",
    "    \n",
    "    # Calculate permutation importance using the sklearn function\n",
    "    r = sklearn_permutation_importance(predict_fn, X, y, \n",
    "                                      n_repeats=n_repeats,\n",
    "                                      random_state=random_state)\n",
    "    \n",
    "    # Create DataFrame with results\n",
    "    perm_importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': r.importances_mean,\n",
    "        'Std': r.importances_std\n",
    "    })\n",
    "    \n",
    "    # Sort by importance\n",
    "    perm_importance_df = perm_importance_df.sort_values('Importance', ascending=False)\n",
    "    \n",
    "    return perm_importance_df\n",
    "\n",
    "def analyze_deeper_layers(model, layer_indices=None):\n",
    "    \"\"\"\n",
    "    Analyze the activation patterns and weights of deeper layers in the network.\n",
    "    \n",
    "    Parameters:\n",
    "    model: Trained neural network model\n",
    "    layer_indices: List of layer indices to analyze, defaults to all layers\n",
    "    \n",
    "    Returns:\n",
    "    dict: Dictionary with layer analysis information\n",
    "    \"\"\"\n",
    "    if layer_indices is None:\n",
    "        layer_indices = range(len(model.layers))\n",
    "    \n",
    "    layer_info = {}\n",
    "    \n",
    "    for i in layer_indices:\n",
    "        layer = model.layers[i]\n",
    "        weights = layer.get_weights()\n",
    "        \n",
    "        if len(weights) == 0:\n",
    "            continue\n",
    "            \n",
    "        # For each layer, calculate weight statistics\n",
    "        layer_info[i] = {\n",
    "            'name': layer.name,\n",
    "            'type': layer.__class__.__name__,\n",
    "            'shape': [w.shape for w in weights],\n",
    "            'weight_stats': {\n",
    "                'mean': float(np.mean(weights[0])),\n",
    "                'std': float(np.std(weights[0])),\n",
    "                'min': float(np.min(weights[0])),\n",
    "                'max': float(np.max(weights[0])),\n",
    "                'sparsity': float(np.mean(np.abs(weights[0]) < 1e-10))\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        if len(weights) > 1:  # Has bias\n",
    "            layer_info[i]['bias_stats'] = {\n",
    "                'mean': float(np.mean(weights[1])),\n",
    "                'std': float(np.std(weights[1])),\n",
    "                'min': float(np.min(weights[1])),\n",
    "                'max': float(np.max(weights[1]))\n",
    "            }\n",
    "    \n",
    "    return layer_info\n",
    "\n",
    "def plot_weight_distributions(model, layer_indices=None):\n",
    "    \"\"\"\n",
    "    Plot the distribution of weights for selected layers.\n",
    "    \n",
    "    Parameters:\n",
    "    model: Trained neural network model\n",
    "    layer_indices: List of layer indices to analyze, defaults to all layers\n",
    "    \n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    if layer_indices is None:\n",
    "        layer_indices = range(len(model.layers))\n",
    "    \n",
    "    # Filter to keep only layers with weights\n",
    "    layers_with_weights = []\n",
    "    for i in layer_indices:\n",
    "        if len(model.layers[i].get_weights()) > 0:\n",
    "            layers_with_weights.append(i)\n",
    "    \n",
    "    if not layers_with_weights:\n",
    "        print(\"No layers with weights found.\")\n",
    "        return\n",
    "    \n",
    "    n_layers = len(layers_with_weights)\n",
    "    fig, axes = plt.subplots(n_layers, 1, figsize=(10, 3*n_layers))\n",
    "    \n",
    "    if n_layers == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i, layer_idx in enumerate(layers_with_weights):\n",
    "        layer = model.layers[layer_idx]\n",
    "        weights = layer.get_weights()[0]\n",
    "        \n",
    "        # Flatten weights for histogram\n",
    "        flat_weights = weights.flatten()\n",
    "        \n",
    "        # Plot histogram\n",
    "        axes[i].hist(flat_weights, bins=50)\n",
    "        axes[i].set_title(f\"Layer {layer_idx}: {layer.name} - Weight Distribution\")\n",
    "        axes[i].set_xlabel(\"Weight Value\")\n",
    "        axes[i].set_ylabel(\"Count\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def analyze_activation_patterns(model, X_sample, layer_indices=None):\n",
    "    \"\"\"\n",
    "    Analyze activation patterns across different layers for a sample of data.\n",
    "    \n",
    "    Parameters:\n",
    "    model: Trained neural network model\n",
    "    X_sample: Sample input data\n",
    "    layer_indices: List of layer indices to analyze, defaults to all layers\n",
    "    \n",
    "    Returns:\n",
    "    dict: Dictionary with activation statistics for each layer\n",
    "    \"\"\"\n",
    "    if layer_indices is None:\n",
    "        layer_indices = range(len(model.layers))\n",
    "    \n",
    "    activation_models = {}\n",
    "    activation_stats = {}\n",
    "    \n",
    "    for i in layer_indices:\n",
    "        try:\n",
    "            # Create a model that outputs the activations from this layer\n",
    "            activation_model = tf.keras.Model(\n",
    "                inputs=model.input,\n",
    "                outputs=model.layers[i].output\n",
    "            )\n",
    "            \n",
    "            # Get activations\n",
    "            activations = activation_model.predict(X_sample, verbose=0)\n",
    "            \n",
    "            # Calculate activation statistics\n",
    "            activation_stats[i] = {\n",
    "                'layer_name': model.layers[i].name,\n",
    "                'mean': float(np.mean(activations)),\n",
    "                'std': float(np.std(activations)),\n",
    "                'min': float(np.min(activations)),\n",
    "                'max': float(np.max(activations)),\n",
    "                'sparsity': float(np.mean(activations < 1e-10)),\n",
    "                'shape': activations.shape\n",
    "            }\n",
    "        except:\n",
    "            # Skip layers that don't support this analysis\n",
    "            continue\n",
    "    \n",
    "    return activation_stats\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Get your model - either load from file or use the existing model in memory\n",
    "    # model = load_model(\"NN3_bs_model.h5\")\n",
    "    try:\n",
    "        model = all_models['bs']['NN3']\n",
    "    except KeyError:\n",
    "        model = loaded_models['bs']['NN3']\n",
    "    # Get feature names\n",
    "    feature_names = feature_columns\n",
    "    \n",
    "    # 1. Analyze weights and show top features\n",
    "    importance_df = analyze_neural_network_weights(model, feature_names, \"NN3 - BS Error Correction\")\n",
    "    \n",
    "    # 2. Plot feature importance\n",
    "    fig = plot_feature_importance(importance_df, \"NN3 - BS Error Correction\", top_n=30)\n",
    "    plt.savefig(\"NN3_bs_feature_importance.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    # 3. Analyze deeper layers\n",
    "    layer_info = analyze_deeper_layers(model)\n",
    "    print(\"\\n=== Layer Information ===\")\n",
    "    for layer_idx, info in layer_info.items():\n",
    "        print(f\"\\nLayer {layer_idx}: {info['name']} ({info['type']})\")\n",
    "        print(f\"  Shape: {info['shape']}\")\n",
    "        print(f\"  Weight stats: Mean={info['weight_stats']['mean']:.4f}, \"\n",
    "              f\"Std={info['weight_stats']['std']:.4f}\")\n",
    "        if 'bias_stats' in info:\n",
    "            print(f\"  Bias stats: Mean={info['bias_stats']['mean']:.4f}, \"\n",
    "                  f\"Std={info['bias_stats']['std']:.4f}\")\n",
    "    \n",
    "    # 4. Plot weight distributions\n",
    "    weight_fig = plot_weight_distributions(model)\n",
    "    plt.savefig(\"NN3_bs_weight_distributions.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    # 5. Analyze activations with a small sample\n",
    "    # (Uncomment if you want to run this analysis)\n",
    "    # sample_size = min(100, len(X_train_scaled))\n",
    "    # X_sample = X_train_scaled[:sample_size]\n",
    "    # activation_stats = analyze_activation_patterns(model, X_sample)\n",
    "    # print(\"\\n=== Activation Statistics ===\")\n",
    "    # for layer_idx, stats in activation_stats.items():\n",
    "    #     print(f\"\\nLayer {layer_idx}: {stats['layer_name']}\")\n",
    "    #     print(f\"  Activation: Mean={stats['mean']:.4f}, Std={stats['std']:.4f}\")\n",
    "    #     print(f\"  Sparsity: {stats['sparsity']:.2%} (percentage of near-zero activations)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acac81a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = test_data[1].sample(n=1000000, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c033939",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[53]\u001b[39m\u001b[32m, line 67\u001b[39m\n\u001b[32m     64\u001b[39m y_test = y_test_dict[grp]\n\u001b[32m     66\u001b[39m \u001b[38;5;66;03m# Compute permutation importance\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m imp_perm = \u001b[43mmanual_permutation_importance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test_scaled\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_columns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     68\u001b[39m title = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgrp.upper()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00march\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Combined Dataset\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     69\u001b[39m plot_importance(imp_perm, top_n=\u001b[32m20\u001b[39m, title=title)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[53]\u001b[39m\u001b[32m, line 24\u001b[39m, in \u001b[36mmanual_permutation_importance\u001b[39m\u001b[34m(model, X, y, feature_names, n_repeats, random_state)\u001b[39m\n\u001b[32m     22\u001b[39m X_permuted = X.copy()\n\u001b[32m     23\u001b[39m X_permuted[:, idx] = rng.permutation(X_permuted[:, idx])\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m permuted_preds = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_permuted\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m.flatten()\n\u001b[32m     25\u001b[39m score = mean_squared_error(y, permuted_preds)\n\u001b[32m     26\u001b[39m scores.append(score - baseline_score)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "def manual_permutation_importance(model, X: np.ndarray, y: np.ndarray,\n",
    "                                  feature_names: list[str],\n",
    "                                  n_repeats: int = 10,\n",
    "                                  random_state: int = 42) -> pd.Series:\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    baseline_preds = model.predict(X, verbose=0).flatten()\n",
    "    baseline_score = mean_squared_error(y, baseline_preds)\n",
    "\n",
    "    importances = []\n",
    "    for idx, col in enumerate(feature_names):\n",
    "        scores = []\n",
    "        for _ in range(n_repeats):\n",
    "            X_permuted = X.copy()\n",
    "            X_permuted[:, idx] = rng.permutation(X_permuted[:, idx])\n",
    "            permuted_preds = model.predict(X_permuted, verbose=0).flatten()\n",
    "            score = mean_squared_error(y, permuted_preds)\n",
    "            scores.append(score - baseline_score)\n",
    "        importances.append(np.mean(scores))\n",
    "\n",
    "    return pd.Series(importances, index=feature_names).sort_values(ascending=False)\n",
    "\n",
    "def plot_importance(importance: pd.Series, top_n: int = 20, title: str = \"Feature Importance\"):\n",
    "\n",
    "    data = importance.head(top_n).sort_values()\n",
    "    plt.figure(figsize=(8, max(4, 0.2 * top_n)))\n",
    "    data.plot.barh()\n",
    "    plt.xlabel('Importance (Increase in MSE)')\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Prepare data once\n",
    "X_test_scaled, y_test_dict, _, feature_names = prepare_data(sample_df, save_paths=False)\n",
    "\n",
    "# Load pre-saved feature columns (if different than current feature_names)\n",
    "feature_columns = joblib.load(r'C:\\Users\\maxva\\OneDrive - Tilburg University\\Msc. Data Science\\Master Thesis\\Code\\Models\\LigthGBM Forecast\\Firm Characteristics\\feature_columns.pkl')\n",
    "\n",
    "groups = ['bs']\n",
    "arches = ['NN3']\n",
    "\n",
    "for grp in groups:\n",
    "    for arch in arches:\n",
    "        try:\n",
    "            model = all_models[grp][arch]\n",
    "        except KeyError:\n",
    "            try:\n",
    "                model = loaded_models[grp][arch]\n",
    "            except KeyError:\n",
    "                print(f\"⚠️ Model not found for group '{grp}' and architecture '{arch}'. Skipping.\")\n",
    "                continue\n",
    "\n",
    "        # Select the correct target (y) for this group\n",
    "        y_test = y_test_dict[grp]\n",
    "\n",
    "        # Compute permutation importance\n",
    "        imp_perm = manual_permutation_importance(model, X_test_scaled.values, y_test.values, feature_columns)\n",
    "        title = f\"{grp.upper()} {arch} Combined Dataset\"\n",
    "        plot_importance(imp_perm, top_n=20, title=title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68efeb0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Sequential name=sequential_5, built=True>\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
